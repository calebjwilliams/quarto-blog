[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Final Project/final project.html",
    "href": "posts/Final Project/final project.html",
    "title": "Recipe Recommendaton Web App",
    "section": "",
    "text": "Project Description\nThe Recipe Recommendation Web App is designed to help users easily find recipes based on the ingredients they have at home and their specific nutritional goals. The app addresses two major challenges in meal planning: ingredient-based recipe discovery and nutrition tracking. The first feature allows users to input the ingredients they currently have in their kitchen or specify a type of meal (e.g., “chicken and rice”), and the app generates a list of recipes that can be made with those ingredients. This helps users minimize food waste by utilizing available ingredients. The second feature is focused on users’ nutritional goals. Users can set specific dietary parameters, such as maximum calorie count or minimum protein intake, and the app will suggest recipes that meet those criteria, ensuring users stay on track with their health goals.\nThe app’s user interface is designed to be intuitive and interactive, enabling users to easily input ingredients, dietary restrictions, and meal preferences. The app connects to a recipe database, which contains detailed information on each recipe, including nutritional data. Users can browse through a list of recipe recommendations that are ranked based on user ratings and relevance to their inputs. They can then select a recipe based on factors like taste preferences, nutritional fit, or ingredient availability. Once a recipe is chosen, users can click on a link to view the full recipe page, which includes step-by-step instructions, ingredient quantities, and nutritional breakdown.\nThe three main technical parts of our project were web scraping recipe and nutritional information from Allreipes.com using Scrapy, storage of scraped and cleaned data within an SQLite3 database, and implementation of a Dash web app to allow for user-friendly input and display of recommended recipes.\nOur project can be found at the following GitHub repository: https://github.com/calebjwilliams/reciperecommendation.\n\n\n\n1. Web Scraping with Scrapy\nThe first step in building the Recipe Recommendation Web App was collecting a comprehensive dataset of recipes. For this purpose, we utilized Scrapy. Scrapy allowed us to automate the process of collecting essential recipe information such as recipe titles, ingredients, preparation times, cooking times, serving sizes, nutritional facts, and user ratings. The scraper leveraged the Scrapy framework alongside Selenium to dynamically load and navigate pages, ensuring all desired data could be captured effectively, even from JavaScript-rendered content.\nThe scraper begins by traversing the recipe index page and extracting links to individual recipe categories. From there, it navigates through recipe listings within each category to reach individual recipe pages, where it extracts structured nutritional and ingredient data we needed using CSS selectors. This multi-layered approach ensures comprehensive data collection.\n\nimport scrapy\nfrom scrapy_selenium import SeleniumRequest\n\nclass AllRecipesSpider(scrapy.Spider):\n    name = 'allrecipes'\n    allowed_domains = ['allrecipes.com']\n    start_urls = ['https://www.allrecipes.com/recipes-a-z-6735880']\n        \n\n    def start_requests(self):\n        yield SeleniumRequest(\n            url=self.start_urls[0],\n            callback=self.parse_index\n        )\n\n    def parse_index(self, response):\n        # Select links for each type of recipe\n        initial_links = response.css('a.mntl-link-list__link::attr(href)').getall()\n        categories = response.css('a.mntl-link-list__link::text').getall()\n        for link, category in zip(initial_links, categories):\n            yield response.follow(link, callback=self.parse_recipe_link, meta={'category': category.strip()})\n\n\n    def parse_recipe_link(self, response):\n        # Extract details for individual recipe pages\n        category = response.meta.get('category', 'Unknown')\n        recipe_links = response.css('a.comp.mntl-card-list-items::attr(href)').getall()\n        for link in recipe_links:\n            if 'www.allrecipes.com/article/' not in link and 'www.allrecipes.com/gallery/' not in link:\n                yield response.follow(link, callback=self.parse_recipe_page, meta={'category': category})\n\n    def parse_recipe_page(self, response):\n        category = response.meta.get('category', 'Unknown')\n        title = response.css('h1.article-heading.text-headline-400::text').get(default='').strip()\n        total_time = response.css('div.mm-recipes-details__item:contains(\"Total Time\") div.mm-recipes-details__value::text').get()\n        prep_time = response.css('div.mm-recipes-details__item:contains(\"Prep Time\") div.mm-recipes-details__value::text').get()\n        cook_time = response.css('div.mm-recipes-details__item:contains(\"Cook Time\") div.mm-recipes-details__value::text').get()\n\n        ingredients = []\n        for ingredient in response.css('li.mm-recipes-structured-ingredients__list-item'):\n            quantity = ingredient.css('span[data-ingredient-quantity=\"true\"]::text').get(default='').strip()\n            unit = ingredient.css('span[data-ingredient-unit=\"true\"]::text').get(default='').strip()\n            name = ingredient.css('span[data-ingredient-name=\"true\"]::text').get(default='').strip()\n            full_ingredient = f\"{quantity} {unit} {name}\".strip()\n            ingredients.append(full_ingredient)\n        ingredients = ', '.join(ingredients) if ingredients else \"No ingredients listed\"\n\n        nutrition_facts = {}\n        nutrition_rows = response.css('tbody.mm-recipes-nutrition-facts-summary__table-body tr.mm-recipes-nutrition-facts-summary__table-row')\n\n        for row in nutrition_rows:\n            nutrient_value = row.css('td.mm-recipes-nutrition-facts-summary__table-cell.text-body-100-prominent::text').get(default='').strip()\n            nutrient_name = row.css('td.mm-recipes-nutrition-facts-summary__table-cell.text-body-100::text').get(default='').strip()\n            if nutrient_value and nutrient_name:\n                nutrition_facts[nutrient_name] = nutrient_value\n \n        serving_info = response.css('p.mm-recipes-serving-size-adjuster__meta::text').get()\n        rating = response.css('div.mm-recipes-review-bar__rating::text').get(default='No rating available').strip()\n        recipe_link = response.url\n        \n\n        yield {\n            'category': category,\n            'title': title,\n            'total_time': total_time,\n            'prep_time': prep_time,\n            'cook_time': cook_time,\n            'ingredients': ingredients,\n            'serving_info': serving_info,\n            'nutrition_facts': nutrition_facts,  # Added nutrition facts to the output\n            'rating': rating,\n            'recipe_link': recipe_link\n        }\n\nHere are the key data points extracted by the web scraper:\n\nCategory: The category or type of recipe (this is what the spider started with. e.g., “Desserts,” “Main Dishes”).\nTitle: The name of the recipe.\nTotal Time: The total estimated time required to prepare the recipe.\nPrep Time: The time required to prepare the ingredients.\nCook Time: The time required for cooking the recipe.\nIngredients: A list of all ingredients with their quantities, units, and names.\nServing Info: Information about the number of servings the recipe yields.\nNutrition Facts: Detailed nutritional information, including calories, fats, carbs, and proteins.\nRating: User rating of the recipe (if available).\nRecipe Link: The URL of the recipe for additional details.\n\nHere’s an example of the pages the scaper naviages through to collect the recipe information. 1. Starts on https://www.allrecipes.com/recipes-a-z-6735880 2. Navigates to each individual recipe link 3. On each individual recipe page, collects recipe information\n\n\n\n\n1a. Data Cleaning\nThe raw dataset underwent several cleaning steps to ensure its quality and usability for the Recipe Recommendation System. Cleaning involved handling missing data, standardizing formats, and transforming fields for easier querying and filtering. Below are the steps involved:\n\nRemoved rows with missing or irrelevant data (e.g., “No ingredients listed”).\nExtracted and standardized nutrition facts, such as calories, fats, carbs, and proteins.\nConverted time-related fields (e.g., “1 hr 10 mins”) into a unified numerical format representing total minutes.\nExtracted serving information and replaced missing or irrelevant ratings.\nStandardized data types for consistent querying.\n\n\n# Drop rows where the ingredients column has \"No ingredients listed\"\ndf_cleaned = df[df['ingredients'] != \"No ingredients listed\"]\n\n# Reset index after dropping rows\ndf_cleaned = df_cleaned.reset_index(drop=True)\n\n# Extract nutrition facts into separate columns\nnutrition_df = df_cleaned['nutrition_facts'].apply(eval).apply(pd.Series)\nnutrition_df.rename(columns=lambda x: x.lower(), inplace=True)\n\n# Merge the extracted nutrition facts back into the main DataFrame\ndf_cleaned = pd.concat([df_cleaned, nutrition_df], axis=1).drop(columns=['nutrition_facts'])\n\n# Extract the numeric value only if it exists, ignoring NaN\ndf_cleaned['serving_info'] = df_cleaned['serving_info'].dropna().str.extract(r'yields (\\d+)(?: servings)?')\n\n# Convert to integer only for non-NaN values\ndf_cleaned['serving_info'] = pd.to_numeric(df_cleaned['serving_info'], errors='coerce')\n\n# Replace \"No rating available\" with NaN\ndf_cleaned['rating'] = df_cleaned['rating'].replace(\"No rating available\", np.nan)\n\n# Convert time to minutes\ndef convert_to_minutes(time_str):\n    \"\"\"\n    Converts a time duration string into the total number of minutes.\n    \n    Parameters:\n    time_str : str\n    \n    Returns:\n    int or None\n    \"\"\"\"\n    if pd.isna(time_str):\n        return None  # Keep NaN if missing\n    # Extend time_map to handle singular and plural forms, including days\n    time_map = {\"day\": 1440, \"days\": 1440, \"hr\": 60, \"hrs\": 60, \"min\": 1, \"mins\": 1}\n\n    # Split the time string into parts and calculate total minutes\n    time_parts = time_str.split()\n    return sum(int(num) * time_map[unit] for num, unit in zip(time_parts[:-1:2], time_parts[1::2]))\n\n# Apply the function to the relevant columns\ndf_cleaned['total_time_mins'] = df_cleaned['total_time'].apply(convert_to_minutes)\n\n# Drop total time, prep time, and cook time columns\ndf_cleaned.drop(columns=['total_time', 'prep_time', 'cook_time'], inplace=True)\n\n# Explicitly convert each column to string, handling edge cases\ncolumns_to_convert = ['category', 'title', 'ingredients', 'recipe_link']\n\nfor col in columns_to_convert:\n    df_cleaned[col] = df_cleaned[col].astype('string')\n\n# Ensure numeric columns are converted to string before applying string operations\nnumeric_columns = ['calories', 'fat', 'carbs', 'protein']\nfor col in numeric_columns:\n    df_cleaned[col] = df_cleaned[col].astype(str)  # Convert to string\n    df_cleaned[col] = pd.to_numeric(\n        df_cleaned[col].str.replace(r'[^\\d.]+', '', regex=True), errors='coerce'\n    )\n\n# Convert numeric columns back to appropriate numeric types\ndf_cleaned['calories'] = df_cleaned['calories'].astype('float64')\ndf_cleaned['fat'] = df_cleaned['fat'].astype('float64')\ndf_cleaned['carbs'] = df_cleaned['carbs'].astype('float64')\ndf_cleaned['protein'] = df_cleaned['protein'].astype('float64')\n\n\n\n\n2. Storage in SQLite Database\nThe second technical aspect of our project involved working with sqlite3. After scraping and cleaning the recipe data, it needed to be stored in an organized format while allowing for quick and efficient access based on user inputs. We chose SQLite because of it’s simple set up and portabliliy—ideal for a small-scale scale application like this one.\nOnce the recipe data was cleaned and preprocessed we inserted it into an SQLite database using a table specifically designed to store recipes. This table includes fields for the recipe title, ingredients, preparation instructions, calorie count, and other relevant details. Each recipe is stored as a row, and its attributes are mapped to corresponding columns. For example, the “ingredients” column contains a string of all ingredients required for the recipe, while the “calories” column holds the nutritional value per serving. By organizing the data in this structured format, the dash app we later implemented can quickly and easliy retrieve necessary recipe information.\n\nimport sqlite3\n\n# connect to SQLite database \nconn = sqlite3.connect('recipes.db')\n\n# save the cleaned DataFrame to a table called 'recipes'\ndf_cleaned.to_sql('recipes', conn, if_exists='replace', index=False)\n\n\nconn.close()\n\nHere’s what the first 10 rows of our database looks like:\n\n\n\n3. Recommendation and Dash Web App Implementation\nThe final component of our project was the implementation of a Dash web app, that allows users to interact with the recipe database and receive recommendations based on their preferences. The implementation queries the SQLite database to retrieve data which is then data is then filtered dynamically using Pandas DataFrame operations in response to user inputs.\nThe Dash app begins with a layout, defined using a combination of html and dcc components. These include text input fields for users to specify their food preferences and dietary restrictions, a dropdown for selecting dietary preferences (e.g., “Vegetarian,” “Vegan”), sliders for numeric range inputs like calorie limits and cooking time, and buttons for initiating searches or requesting surprise recipes. Each input field is styled for usability and designed to gather specific parameters that feed into the recommendation logic.\n\n# App layout\napp.layout = html.Div([\n    html.H1(\"Recipe Recommendation System\", style={'text-align': 'center', 'color': '#4CAF50'}),\n\n    html.Hr(),\n\n    # Ingredients input\n    html.Div([\n        html.Label(\"Enter Food Preference:\"),\n        dcc.Input(id='ingredients-input', type='text', placeholder='e.g., chicken, garlic, olive oil',\n                  style={'width': '100%', 'padding': '10px'}),\n    ], style={'margin-bottom': '10px'}),\n\n    # Dietary restrictions\n    html.Div([\n        html.Label(\"Enter Any Dietary Restrictions:\"),\n        dcc.Input(id='restrictions-input', type='text', placeholder='e.g., milk, peanuts, sesame',\n                  style={'width': '100%', 'padding': '10px'}),\n    ], style={'margin-bottom': '10px'}),\n\n    # Dietary preferences dropdown\n    html.Div([\n        html.Label(\"Dietary Preferences:\"),\n        dcc.Dropdown(\n            id='dietary-preferences-dropdown',\n            options=[\n                {'label': 'Vegetarian', 'value': 'vegetarian'},\n                {'label': 'Vegan', 'value': 'vegan'},\n                {'label': 'Gluten-Free', 'value': 'gluten-free'},\n                {'label': 'Keto', 'value': 'keto'},\n            ],\n            placeholder=\"Select dietary preferences\",\n            multi=True,\n            style={'cursor': 'pointer'}\n        )\n    ], style={'margin-bottom': '10px'}),\n\n    html.Div([\n        html.Div([\n            # Left side: Calorie and Protein Range\n            html.Div([\n                html.Label(\"Calorie Range:\", style={'width': '140px', 'text-align': 'right', 'margin-right': '10px'}),\n                dcc.Input(id='calorie-min', type='number', placeholder='Min', style={'margin-right': '10px', 'width': '70px'}),\n                dcc.Input(id='calorie-max', type='number', placeholder='Max', style={'width': '70px'}),\n            ], style={'display': 'flex', 'align-items': 'center', 'margin-bottom': '10px'}),\n    \n            html.Div([\n                html.Label(\"Protein Range:\", style={'width': '140px', 'text-align': 'right', 'margin-right': '10px'}),\n                dcc.Input(id='protein-min', type='number', placeholder='Min', style={'margin-right': '10px', 'width': '70px'}),\n                dcc.Input(id='protein-max', type='number', placeholder='Max', style={'width': '70px'}),\n            ], style={'display': 'flex', 'align-items': 'center', 'margin-bottom': '10px'}),\n        ], style={'width': '45%', 'display': 'inline-block'}),\n    \n        html.Div([\n            # Right side: Carbs and Fat Range\n            html.Div([\n                html.Label(\"Carbs Range:\", style={'width': '140px', 'text-align': 'right', 'margin-right': '10px'}),\n                dcc.Input(id='carbs-min', type='number', placeholder='Min', style={'margin-right': '10px', 'width': '70px'}),\n                dcc.Input(id='carbs-max', type='number', placeholder='Max', style={'width': '70px'}),\n            ], style={'display': 'flex', 'align-items': 'center', 'margin-bottom': '10px'}),\n    \n            html.Div([\n                html.Label(\"Fat Range:\", style={'width': '140px', 'text-align': 'right', 'margin-right': '10px'}),\n                dcc.Input(id='fat-min', type='number', placeholder='Min', style={'margin-right': '10px', 'width': '70px'}),\n                dcc.Input(id='fat-max', type='number', placeholder='Max', style={'width': '70px'}),\n            ], style={'display': 'flex', 'align-items': 'center', 'margin-bottom': '10px'}),\n        ], style={'width': '45%', 'display': 'inline-block', 'margin-left': '5px'}),  \n    ], style={'display': 'flex', 'justify-content': 'space-between', 'align-items': 'flex-start'}),\n\n    # Cooking time slider\n    html.Div([\n        html.Label(\"Maximum Cooking Time (minutes):\"),\n        dcc.Slider(\n            id='cooking-time-slider',\n            min=10,\n            max=150,  \n            step=5,\n            value=30,\n            marks={10: '10', 30: '30', 60: '60', 90: '90', 120: '120', 150: '150'},  \n            tooltip={\"placement\": \"bottom\", \"always_visible\": False}\n        ),\n    ], style={'margin-bottom': '10px'}),\n\n\n    # Max Search Results\n    html.Div([\n        html.Label(\"Max Search Results:\", style={'margin-right': '10px', 'font-weight': 'bold'}),\n        dcc.Input(id='search-input', type='number', min=1, max=25, step=1, value=10,\n                  style={'width': '70px'}),\n    ], style={'display': 'flex', 'align-items': 'center', 'margin-bottom': '10px'}),\n\n    # Submit button\n    html.Div([\n        html.Button('Search Recipes', id='search-button',\n                    style={'text-align': 'center', 'background-color': '#4CAF50', 'color': 'white', 'padding': '10px 20px', 'border': 'none', 'cursor': 'pointer'}),\n    ], style={'text-align': 'center', 'padding': '5px'}),\n\n\n    # Surprise me button\n    html.Div([\n        html.Button('Surprise Me', id='surprise-me', style={'text-align': 'center', 'background-color': '#4CAF50', 'color': 'white', 'padding': '10px 20px', 'border': 'none', 'cursor': 'pointer'})\n    ], style={'text-align': 'center', 'padding': '5px'}),\n\n    html.Hr(),\n\n    \n    # Placeholder for displaying results\n    html.Div(id='recipe-results')\n])\n\nHere’s the final layout of our Dash App:\n\n\nData Filtering\nThe filtering–i.e., recommendation–starts by determining which button triggered the callback. If the user clicks “Surprise Me,” the app bypasses all input-based filtering and randomly selects a specified number of recipes (e.g., 5). This feature allows users to discover recipes they may not have actively searched for, making the app more engaging and exploratory.\nFor the “Search Recipes” functionality, the callback function processes the following user inputs:\n\nDietary Preferences: The app filters recipes based on selected preferences (e.g., “Vegetarian,” “Vegan”). Recipes are matched by searching for these terms in the recipe category column.\nRestrictions: The app excludes recipes containing restricted ingredients by ensuring none of the specified terms are found in the ingredients column.\nIngredients: Recipes are filtered to include only those that match the specified ingredients. The ingredients column is searched for any of the user-specified keywords.\nCooking Time: Recipes exceeding the maximum cooking time set by the user are excluded.\n\nAfter applying all filters, the DataFrame is shuffled and sorted by user ratings or other criteria to ensure a randomized yet relevant set of recommendations is presented. The filtered data is then displayed dynamically on the web app.\nAdditionally, if no recipes match the user’s criteria, the app displays a friendly message informing the user that their preferences yielded no results.\n\n# Callbacks for functionality\n@app.callback(\n    Output('recipe-results', 'children'),\n    [Input('search-button', 'n_clicks'), Input('surprise-me', 'n_clicks')],\n    State('ingredients-input', 'value'),\n    State('restrictions-input', 'value'),\n    State('dietary-preferences-dropdown', 'value'),\n    State('calorie-min', 'value'),\n    State('calorie-max', 'value'),\n    State('carbs-min', 'value'),\n    State('carbs-max', 'value'),\n    State('protein-min', 'value'),\n    State('protein-max', 'value'),\n    State('fat-min', 'value'),\n    State('fat-max', 'value'),\n    State('cooking-time-slider', 'value'),\n    State('search-input', 'value')\n)\ndef recommend_recipes(search_clicks, surprise_clicks, ingredients, restrictions, dietary_preferences,\n                      cal_min, cal_max, prot_min, prot_max, fat_min, fat_max, carbs_min, carbs_max, \n                      max_time, max_results):\n    \"\"\"\n    Generates a list of recipe recommendations based on user inputs and preferences.\n\n    This callback function is triggered when the user clicks either the \"Search Recipes\"\n    or \"Surprise Me\" button. It retrieves recipes from an SQLite database, applies various\n    filters based on user inputs (e.g., ingredients, dietary preferences, macronutrient ranges),\n    and returns the results as an interactive HTML list.\n\n    Parameters:\n    search_clicks : int\n        Number of times the \"Search Recipes\" button has been clicked.\n    surprise_clicks : int\n        Number of times the \"Surprise Me\" button has been clicked.\n    ingredients : str\n        A comma-separated list of desired ingredients to include in the recipes.\n    restrictions : str\n        A comma-separated list of restricted ingredients to exclude from the recipes.\n    dietary_preferences : list of str\n        A list of dietary preferences (e.g., 'Vegetarian', 'Vegan').\n    cal_min : int\n        The minimum calorie value for recipes.\n    cal_max : int\n        The maximum calorie value for recipes.\n    prot_min : int\n        The minimum protein value for recipes (in grams).\n    prot_max : int\n        The maximum protein value for recipes (in grams).\n    fat_min : int\n        The minimum fat value for recipes (in grams).\n    fat_max : int\n        The maximum fat value for recipes (in grams).\n    carbs_min : int\n        The minimum carbohydrate value for recipes (in grams).\n    carbs_max : int\n        The maximum carbohydrate value for recipes (in grams).\n    max_time : int\n        The maximum cooking time for recipes (in minutes).\n    max_results : int\n        The maximum number of recipes to display in the results.\n\n    Returns:\n    html.Div\n        A Dash HTML Div containing the list of recommended recipes.\n\n    \"\"\"\"\n    \n    # Check if no buttons have been clicked\n    if not search_clicks and not surprise_clicks:\n        return html.Div(\n            \"Enter your preferences and click 'Search Recipes.'\", \n            style={'text-align': 'center', 'padding': '5px'}\n        )\n    \n    # Identify which button triggered the callback\n    ctx = callback_context\n    button_id = ctx.triggered[0]['prop_id'].split('.')[0]\n\n    # Proceed only if a button was actually clicked\n    if button_id not in ['search-button', 'surprise-me']:\n        return html.Div(\n            \"Enter your preferences and click 'Search Recipes.'\", \n            style={'text-align': 'center', 'padding': '5px'}\n        )\n\n    # Connect to SQLite database\n    conn = sqlite3.connect('recipes.db')\n    query = \"SELECT * FROM recipes\"\n    df = pd.read_sql(query, conn)\n\n    if button_id == 'surprise-me':\n        # \"Surprise Me\" button clicked\n        df = get_random_recipes(df, max_results)\n    else:\n        # \"Search Recipes\" button clicked\n\n        # 1. Filter by dietary preferences\n        if dietary_preferences:\n            df = apply_dietary_restrictions(dietary_preferences, df)\n\n        # 2. Exclude restricted items\n        if restrictions:\n            df = exclude_restricted_items(restrictions, df)\n\n        # 3. Filter by desired ingredients\n        if ingredients:\n            df = filter_by_preferences(ingredients, df)\n        \n        # 4. Apply numeric range filters\n        df = filter_by_ranges(df, cal_min, cal_max, prot_min, prot_max, fat_min, fat_max, carbs_min, carbs_max, max_time)\n\n        # Shuffle the DataFrame after applying filters\n        df = df.sample(frac=1).reset_index(drop=True)  # Shuffle rows\n\n        # Sort by reviews (rating) and limit results\n        df = df.sort_values('rating', ascending=False).head(max_results)\n\n    # Close database connection\n    conn.close()\n\n    # Handle empty results\n    if df.empty:\n        return html.Div(\n            \"No recipes matched your preferences!\", \n            style={'text-align': 'center', 'padding': '5px', 'color': '#fc0345'}\n        )\n\n    # Display\n    return html.Div([\n        html.Div([\n            html.H3(row['title'], style={'color': '#4CAF50'}),\n            html.P(f\"Category: {row['category']}\"),\n            html.P(f\"Calories: {row['calories']} | Protein: {row['protein']}g | Carbs: {row['carbs']}g | Fat: {row['fat']}g (Per Serving)\"),\n            html.P(f\"Rating: {convert_rating_to_stars(row['rating'])}\"),\n            html.P(f\"Cooking Time: {row['total_time_mins']} minutes\"),\n            # Format ingredients with each line starting with a number\n            html.Div([\n                html.Span(\"Ingredients:\", style={'font-weight': 'bold'}),\n                html.Ul([\n                    html.Li(ingredient.strip()) \n                    for ingredient in row['ingredients'].split(', ') \n                    if ingredient.strip()[0].isdigit() or not ingredient.strip()[0].isdigit()\n                ], style={'margin-top': '10px'})\n            ]),\n            html.P(f\"Servings: {int(row['serving_info']) if pd.notnull(row['serving_info']) else 'N/A'}\"),  \n            html.A(\"View Recipe\", href=row['recipe_link'], target=\"_blank\", style={'color': '#4CAF50', 'text-decoration': 'underline'})\n        ], style={'margin-bottom': '20px'})\n        for _, row in df.iterrows()\n    ])\n\nHere’s an example output when a user searches with the following inputs: - Food Preference: ‘chicken’ - Dietary Restriction: ‘peanuts’ - Dietary Preference: ‘Gluten Free’ - Max Cook Time: ‘60 Minutes’\n\n\n\n\nConcluding Remarks\nThe Recipe Recommendation System we created showcases the power of data-driven personalization in everyday cooking. Initially, the project aimed to incorporate natural language processing (NLP) to generate robust recommendations based on similarity scores, matching recipes to user input with higher accuracy. However, integrating NLP into the Dash interface proved challenging due to computational constraints and compatibility issues. Despite this, the system’s macro-based content filtering successfully allows users to discover recipes tailored to their nutritional preferences, dietary needs, and ingredient availability.\nThe incorporation of SQLite played a vital role in managing the cleaned dataset efficiently. By storing the data in a structured format, SQLite enabled fast and reliable querying for recommendations based on user input. This lightweight database ensured seamless integration with the Dash app, allowing real-time filtering and retrieval of recipes without the need for an overly complex backend infrastructure.\nWith more time or resources, exploring alternative embedding models optimized for Dash could enable seamless NLP integration, making the recommendations even more precise. We can also explore ways to incorporate other recipe websites for more recipes available to be recommended. Furthermore, the current system addresses important ethical considerations:\nData Privacy: All data was collected from publicly available sources adhered to terms and conditions to respect data ownership.\nBias in Recommendations: Efforts were made to include diverse recipes and avoid biases toward specific cuisines or dietary preferences, ensuring fair and inclusive recommendations for a wide range of users.\nAccessibility: The app is designed with an inutitive and easy to use design with user-friendliness in mind, ensuring easy accessibility.\nOverall, the current implementation demonstrates strong performance in creating valuable recipe suggestions, while leaving room for future enhancements to improve precision and user experience further."
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "NOAA Climate Data Graphics",
    "section": "",
    "text": "Today I’m going to be looking at the NOAA climate data we’ve been using in class, and after combining the data and storing it in a sqlite database, create several interesting, interactive data graphics!\n\n1. Create a Database\nThe first step is to set up the sqlite3 database, so I’ll use import to retrieve the necessary packages.\n\nimport pandas as pd\nimport sqlite3 \n\nI’ll start by creating a folder called “datafiles” and retrieve the climate data from the github url, storing the files in the folder\n\n# create directory called 'datafiles'\nimport os\nif not os.path.exists(\"datafiles\"): # only if doesn't already exist\n    os.mkdir(\"datafiles\")\n\n# download the files\nimport urllib.request\nintervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nNow I can finally create our database using sqlite’s .connect() function, which establishes a connection to the database, hence the variable name “conn”.\nI’ll name the database “climate.db”.\n\nconn = sqlite3.connect(\"climate.db\")\n\nNext, I’ll define a function prepare_df(df) which takes in a dataframe and cleans and reformats it from wide to long format.\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n\n    return df\n\nNow I’ll use that function and add a table called ‘temperatures’ to my database.\n\nintervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df)\n    df = df.dropna() # deal with NaN temp values\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nHere I’m reading in the data files one by one, reformatting them with prepare_df(df), dropping any NaNs, and then using sqlite’s .to_sql() function to add them (appending once the table is created) to a table called ‘temperatures’.\nNext I’ll add two more tables, ‘stations’ and ‘countries’ from the data retrieved from the github urls.\n\n# Add stations table\n# retrieve the stations data from the url and read into a pandas dataframe\nstations_url = \"https://raw.githubusercontent.com/PIC16B-ucla/24F/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(stations_url)\n\n# deal with NaN values\nstations = stations.dropna()\n\n# send to database\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\n\n# Add countries table\n# retrieve the countries data from the url and read into a pandas dataframe\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\n\n# deal with NaN values\ncountries = countries.dropna()\n\n# send to database\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n278\n\n\nNow let’s check if the three tables I created were added to the database correctly. I’ll use .execute() to run a SQL command that selects the name of each table in the database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nNice! Let’s also check the contents of each table.\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWith the database created and tables added, I can move onto the next step.\n\n\n2. Write a Query Function\nIn a separate .py file, I’ve writted a function query_climate_database() that takes a database file, country, start year, end year, and month, and queries the database to retrieve NAME, LATITUDE,LONGITUDE, Country, Year, Month, and Temp.\n\nimport sqlite3\nimport pandas as pd\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    # establish connection to database\n    conn = sqlite3.connect(db_file)\n\n    \n    # define SQL query\n    cmd = f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name AS Country, T.Year, T.Month, T.Temp \n    FROM stations S\n    INNER JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\"\n    INNER JOIN temperatures T ON T.ID = S.ID\n    WHERE C.Name = '{country}'\n    AND T.Year BETWEEN {year_begin} AND {year_end}\n    AND T.Month = {month}\n    ORDER BY S.NAME\n    \"\"\"\n    # send SQL query to database, store in dataframe\n    df = pd.read_sql_query(cmd, conn)\n    \n    # close connection\n    conn.close()\n\n    return df \n\nNote how with INNER JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\" I’m joining countries’ FIPS 10-4 column with the first two chars of stations’ ID (which corresponds to FIPS 10-4).\n\n# import query_climate_database and check its contents\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    # establish connection to database\n    conn = sqlite3.connect(db_file)\n\n    \n    # define SQL query\n    cmd = f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name AS Country, T.Year, T.Month, T.Temp \n    FROM stations S\n    INNER JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\"\n    INNER JOIN temperatures T ON T.ID = S.ID\n    WHERE C.Name = '{country}'\n    AND T.Year BETWEEN {year_begin} AND {year_end}\n    AND T.Month = {month}\n    ORDER BY S.NAME\n    \"\"\"\n    # send SQL query to database, store in dataframe\n    df = pd.read_sql_query(cmd, conn)\n    \n    # close connection\n    conn.close()\n\n    return df \n\n\n\nLet’s test that the query_climate_database() function works correctly\n\ndf = query_climate_database(db_file = \"climate.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\ndf.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.25\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.25\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.25\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.25\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.25\nIndia\n1988\n1\n19.54\n\n\n\n\n\n\n\n\n\n3. Write a Geographic Scatter Function for Yearly Temperature Increases\nNow with my database setup and a query function to select the data I want, I’ll try to answer the question ‘How does the average yearly change in temperature vary within a given country?’ by making a geographic scatter function for yearly temperature increases.\n\nimport plotly.express as px\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nMy function temperature_coefficient_plot() takes in 7 arguments: db_file, country, year_begin, year_end, month, min_obs, **kwargs(additional arguments for the scatter_mapbox.\n- It first uses the query_climate_database() function to retrieve the necessary data from the database and filter it based on the min_obs argument .\n- Next, I define a function called slope_coeffieicent which calculates the linear regression coeficient, or how much temperature is expected to change per year, at a given station.\n- Using that function, I then apply it to each station within my data to get the coefficients for each stations\n- Then, I create the scatter_mapbox, assigning the color to the estimated yearly increase (coefficients), and add a few settings like carto-positron as the default mapbox_style, and zoom.\n- Finally, I return the scatter_mapbox figure.\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    # get data from SQL query\n    df = query_climate_database(db_file = \"climate.db\",\n                           country = \"India\", \n                           year_begin = 1980, \n                           year_end = 2020,\n                           month = 1)\n    # select stations with &gt;= min_obs years of data\n    station_count = df.groupby('NAME')['Year'].count()\n    stations = df[df['NAME'].isin(station_count[station_count &gt;= min_obs].index)].copy()\n    \n    # function to calculate the linear regression coefficient (how much temp changed per year) at a given station\n    def slope_coefficient(stations):\n        X = stations['Year'].values.reshape(-1, 1) # Year as X axis\n        y = stations['Temp'].values # Temp as Y axis\n        model = LinearRegression()\n        model.fit(X, y)\n        return pd.Series({'Estimated_Yearly_Increase': model.coef_[0]}) # return coefficient\n    \n    # get the coefficients for all stations and add to stations df\n    station_slopes = stations.groupby(\"NAME\")[['Year', 'Temp']].apply(slope_coefficient).reset_index()\n    stations = stations.merge(station_slopes[['NAME', 'Estimated_Yearly_Increase']], on='NAME', how='left')\n    # deal with NaNs\n    stations = stations.dropna(subset=['Estimated_Yearly_Increase'])\n\n    \n    #create scatter_mapbox plot \n    fig = px.scatter_mapbox(\n        stations,\n        lat = 'LATITUDE',\n        lon = 'LONGITUDE',\n        color = 'Estimated_Yearly_Increase',\n        hover_name = 'NAME',\n        hover_data = {'Estimated_Yearly_Increase': ':.4f'},  # rount to 4 decimal places\n        color_continuous_midpoint = 0,\n        **kwargs\n    )\n\n    # mapbox_settings\n    fig.update_layout(\n        mapbox_style=kwargs.get('mapbox_style', 'carto-positron'), \n        mapbox_zoom=kwargs.get('zoom', 2),\n        title = f\"Estimates of yearly increase in temperature in Month {month} for stations in {country}, years {year_begin} - {year_end}\",\n        coloraxis_colorbar = dict(title=\"Estimated Yearly Increase (°C)\")\n    )\n    \n    return fig\n\nI’ll test my function to see yearly increase in India in January in years 1980 to 2020. The color_map is an additional argument passed via **kwargs.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\",\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\nVoila! I now have a geographic visualization of the average yearly temperature increases within a given country.\n\n\nFigure # 2\nNow I’ll make another graphic and try to answer the question: ‘What is the frequency of extreme temperatures over time within a given country?’\nI’ll start by writing a new query function, this time to select NAME, LATITUDE, LONGITUDE, Year, and Temp data from my database.\n\ndef query_extreme_temps(db_file, country, year_begin, year_end):\n    # define the query\n    cmd = f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, T.Year, T.Temp\n    FROM stations S\n    INNER JOIN temperatures T ON S.ID = T.ID\n    INNER JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\"\n    WHERE C.Name = '{country}' AND \n    T.Year BETWEEN {year_begin} AND {year_end} \n    ORDER BY T.Year\n    \"\"\"\n    # Connect to the database and execute the query\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(cmd, conn)\n    \n    # Calculate the 90th and 10th percentiles (above and below are extreme)\n    high_threshold = df['Temp'].quantile(0.90)\n    low_threshold = df['Temp'].quantile(0.10)\n        \n    return df, high_threshold, low_threshold\n\nNotice that my function I also calculated and returned two vectors, high_threshold and low_threshold. Since I’m trying to analyze extreme temperatures over time, I’ve defined extreme temperatures as anything above the 90th percentile and below the 10 percentile of temperatures.\nLet’s test that my query function works.\n\ndf, high, low = query_extreme_temps(\"climate.db\", \"Brazil\", 1990, 2020)\ndf.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nTemp\n\n\n\n\n0\nFERNANDO_DE_NORONHA\n-3.850\n-32.417\n1990\n27.38\n\n\n1\nCONCEICAO_DO_ARAGUAIA\n-8.250\n-49.280\n1990\n27.44\n\n\n2\nCUIABA\n-15.550\n-56.117\n1990\n27.44\n\n\n3\nRIO_DE_JANEIRO\n-22.917\n-43.167\n1990\n28.94\n\n\n4\nPORTO_DE_MOZ\n-1.733\n-52.233\n1990\n27.38\n\n\n\n\n\n\n\n\nhigh\n\n28.49\n\n\n\nlow\n\n18.99\n\n\nGreat. So for Brazil from 1990 to 2020, anything above 28.49 degrees and anything below 18.99 degrees is extreme. Let’s define a function to calculate the frequency of these extreme events.\n\ndef extreme_temp_frequency(df, high_threshold, low_threshold):\n    # classify temperatures as extreme high or low\n    df['Extreme_High'] = df['Temp'] &gt; high_threshold\n    df['Extreme_Low'] = df['Temp'] &lt; low_threshold\n\n    # count the number of extreme temperatures by year\n    yearly_extremes = df.groupby('Year').agg(\n        Extreme_Highs=('Extreme_High', 'sum'),\n        Extreme_Lows=('Extreme_Low', 'sum')\n    ).reset_index()\n\n    return yearly_extremes\n\nNow I can define my plotting function. I’ll have it take 5 arguments: db_file, country, year_begin, year_end.\n- First it retrieves the dataframe, high and low thresholds from the query function.\n- Next, it passes those to the extreme_temp_frequency() function to get a dataframe with the frequency of extreme events.\n- Then, using px.line() it plots Year as the independent variable vs Frequency as the dependent variable, creating one line for Extreme highs and one for Extreme lows.\nFinally, it updates some settings like hovertemplate and legend_title, and then returns the figure.\n\ndef plot_extreme_temp_frequency(db_file, country, year_begin, year_end):\n    # get data from SQL query\n    df, high_threshold, low_threshold = query_extreme_temps(db_file, country, year_begin, year_end)\n    \n    # get frequency of extreme temperatures per year\n    yearly_extremes = extreme_temp_frequency(df, high_threshold, low_threshold)\n\n\n    # create line plot\n    fig = px.line(\n        yearly_extremes, \n        x='Year', \n        y= ['Extreme_Highs', 'Extreme_Lows'], \n        category_orders={\"Event Type\": [\"Extreme_Lows\", \"Extreme_Highs\"]}, # maintains order for coloring\n        title=f\"Frequency of Extreme Temperatures in {country}, ({year_begin}-{year_end})\",\n        labels={'value': 'Number of Extreme Events', 'variable': 'Event Type'},\n        markers=True,\n        height = 500\n    )\n\n    # set hover to number of events \n    fig.update_traces(hovertemplate='%{y}')\n\n    # line plot settings \n    fig.update_layout(\n        yaxis_title=\"Number of Extreme Temperature Events\",\n        xaxis_title=\"Year\",\n        legend_title=\"Event Type\",\n        hovermode= \"x\",\n        hoverlabel=dict(\n            bgcolor=\"white\",\n            font_size=16,\n            font_family=\"Rockwell\"\n        )\n    )\n    \n    return fig\n\nLets see what the extreme events were like in Brail from 1990 to 2020.\n\nplot_extreme_temp_frequency(\"climate.db\", \"Brazil\", 1990, 2020)\n\n\nOkay. Not bad. I think it’s a little hard to read, so I’ll adjust it using facet_row so that there are two graphs, one for extreme highs and one for extreme lows\n\ndef plot_extreme_temp_frequency(db_file, country, year_begin, year_end):\n    # get data from SQL query\n    df, high_threshold, low_threshold = query_extreme_temps(db_file, country, year_begin, year_end)\n    \n    # get frequency of extreme temperatures per year\n    yearly_extremes = extreme_temp_frequency(df, high_threshold, low_threshold)\n    \n    # Melt data for easier faceting\n    yearly_extremes = yearly_extremes.melt(id_vars='Year', value_vars=['Extreme_Highs', 'Extreme_Lows'],\n                                                  var_name='Event Type', value_name='Number of Events')\n\n\n    # create line plot\n    fig = px.line(\n        yearly_extremes, \n        x='Year', \n        y= 'Number of Events',\n        color='Event Type', # set color based on highs/lows\n        category_orders={\"Event Type\": [\"Extreme_Lows\", \"Extreme_Highs\"]},\n        title=f\"Frequency of Extreme Temperatures in {country}, ({year_begin}-{year_end})\",\n        labels={'value': 'Number of Extreme Events', 'variable': 'Event Type'},\n        markers=True,\n        facet_row = 'Event Type', # one graph for each event type (extreme high/extreme low)\n        height = 700\n    )\n\n    # set hover to number of events \n    fig.update_traces(hovertemplate='%{y}')\n\n    # line plot settings \n    fig.update_layout(\n        yaxis_title=\"Number of Extreme Temperature Events\",\n        xaxis_title=\"Year\",\n        legend_title=\"Event Type\",\n        hovermode= \"x\",\n        hoverlabel=dict(\n            bgcolor=\"white\",\n            font_size=16,\n            font_family=\"Rockwell\"\n        )\n    )\n    \n    return fig\n\n\nplot_extreme_temp_frequency(\"climate.db\", \"Brazil\", 1990, 2020)\n\nNow lets see the Brazil’s climate data again.\n\nCool! It looks there’s a rough pattern where the years that had extreme highs also had extreme lows!\n\n\nFigure #3\nFor my next graphic I want to explore the question: ‘What’s the relationship between latitude and elevation on temperatures?’\nLike before, I’ll start by defining a new query function. Here I’m interested in Country, Station, Elevation, Latitude, Year, and Temp.\n\n# create new query function\ndef query_temp_elevation(db_file, country, year_begin, year_end):\n    cmd = f\"\"\"\n    SELECT C.Name AS Country, S.NAME AS Station, S.STNELEV AS Elevation, S.LATITUDE AS Latitude, T.Year, T.Temp\n    FROM temperatures T\n    JOIN stations S ON T.ID = S.ID\n    JOIN countries C ON SUBSTR(S.ID, 1, 2) = C.\"FIPS 10-4\"\n    WHERE C.Name = '{country}' AND T.Year BETWEEN {year_begin} AND {year_end}\n    GROUP BY S.NAME, T.Year\n    ORDER BY T.Year;\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(cmd, conn)\n    \n    # Classify elevation types (low, middle, high) using quantiles \n    elevation_quantiles = df[\"Elevation\"].quantile([0.33, 0.66])\n    \n    def classify_elevation(elevation):\n        if elevation &lt;= elevation_quantiles[0.33]:\n            return 'Low'\n        elif elevation &lt;= elevation_quantiles[0.66]:\n            return 'Middle'\n        else:\n            return 'High'\n    \n    df[\"Elevation_Type\"] = df[\"Elevation\"].apply(classify_elevation)\n    \n    return df\n\nI want to look at 3 variables at once, latitude and elevation, and temperature, so I plan to use a scatterplot. But first I need to make elevation into a categorical variable. In my query function, I classify each elevation as one of 3 types (low, middle, or high) using the .quantile function like before, and add a new column to my data called “Elevation_Type”.\nLet’s check if it works.\n\ndf = query_temp_elevation(\"climate.db\", \"China\", 1990, 2000)\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nStation\nElevation\nLatitude\nYear\nTemp\nElevation_Type\n\n\n\n\n0\nChina\nABAG_QI\n1128.0\n44.017\n1990\n-24.07\nHigh\n\n\n1\nChina\nAIHUI\n166.0\n50.250\n1990\n-24.75\nLow\n\n\n2\nChina\nAKQI\n2300.0\n40.780\n1990\n-7.20\nHigh\n\n\n3\nChina\nALAR\n1013.0\n40.500\n1990\n-6.88\nMiddle\n\n\n4\nChina\nALTAY\n737.0\n47.733\n1990\n-14.42\nMiddle\n\n\n\n\n\n\n\nNext I can define my plotting function taking the following inputs: db_file, country, year_begin, year_end.\n- First I use the query function to get the database.\n- then using the px.scatter function, I create a scatterplot setting Latitude as the X axis, Temp as Y, and color based on the Elevation Type.\n- like before I use facets to create 3 separate plots, one for each elevation type.\n- Lastly, I add a trendline, update the layout, and return the figure.\n\ndef plot_latitude_temp_elevation(db_file, country, year_begin, year_end):\n        df = query_temp_elevation(db_file, country, year_begin, year_end)\n\n        fig = px.scatter(df,\n                         x=\"Latitude\", \n                         y=\"Temp\",\n                         color=\"Elevation_Type\",\n                         opacity=0.6, \n                         height=700,\n                         facet_row=\"Elevation_Type\", # facet by Elevation type\n                         category_orders={\"Elevation_Type\": [\"High\", \"Middle\", \"Low\"]}, # ensures facets are shown in this order\n                         labels={\"Temp\": \"Temperature (°C)\", \"Latitude\": \"Latitude (°)\", \"Elevation_Type\": \"Elevation Level\"},\n                         title=f\"Correlation Between Latitude, Elevation, and Temperature in {country} ({year_begin}-{year_end})\",\n                         trendline=\"ols\") # add a trendline \n        \n        fig.update_layout(\n            title=dict(x=0.5, xanchor=\"center\", font=dict(size=16)),\n            legend=dict(title=\"Elevation Level\", orientation=\"h\", y=1.0, x=0.5, xanchor=\"center\"),\n        )\n        \n        fig.update_yaxes(title_text=\"Temperature (°C)\", tickfont=dict(size=10))\n        fig.update_xaxes(title_text=\"Latitude (°)\", tickfont=dict(size=10))\n\n        return fig\n\n\nfig = plot_latitude_temp_elevation(\"climate.db\", \"China\", 1990, 2020)\nfig.show()\n\nLet’s see how latitude and elevation affect temperature in China from 1990-2020.\n\nHow about Canada from 2000-2010?\n\nfig = plot_latitude_temp_elevation(\"climate.db\", \"Canada\", 2000, 2010)\nfig.show()\n\n\nThere’s quite a bit we can learn from these graphs!\n- Across all elevation levels, there is a negative correlation between latitude and temperature. As latitude increases (moving further north), temperatures generally decrease.\n- High Elevation: The temperature values are mostly clustered below 0°C, with many points falling below -20°C, suggesting that high-elevation areas are significantly cooler overall.\n- Middle Elevation: The temperature range is more moderate, roughly between 0°C and 20°C, indicating a more temperate climate at middle elevations.\n- Low Elevation: Temperatures are generally warmer, with most values above 0°C and some reaching over 20°C, showing how lower elevations tend to be warmer."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "",
    "text": "Hello, today we’ll be simulating 2D heat diffusion using various methods, including matrix multiplication, sparse matrices with JAX, direct numpy operations, and JAX with just-in-time (JIT) compilation!"
  },
  {
    "objectID": "posts/HW4/index.html#building-the-finite-difference-matrix",
    "href": "posts/HW4/index.html#building-the-finite-difference-matrix",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Building the Finite Difference Matrix",
    "text": "Building the Finite Difference Matrix\nTo construct the matrix A, we define a function get_A(N) in a separate heat_equation.py file that returns an N^2 x N^2 matrix:\n\nfrom heat_equation import advance_time_matvecmul\nfrom heat_equation import get_A\nimport inspect\n\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"Constructs 2d finite difference matrix A for simulating 2D heat diffusion \n    on an N x N grid using matrix-vector multiplication.\n    \n    Args:\n        N: (int) size of one dimension of the N x N grid.\n        \n    Returns:\n        A: (numpy.ndarray) 2d finite difference matrix of size N^2 x N^2.\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    \n    return A\n\n\n\n\nn = N * N: Computes the total number of grid points, flattening the N x N grid to a vector of length N^2.\ndiagonals = [...]: Creates arrays for each diagonal in the finite difference matrix. The main diagonal has values of -4, representing the central point in the heat equation, while neighboring diagonals have values of 1.\ndiagonals[1][(N-1)::N] = 0: Ensures that the diagonals wrap correctly, applying boundary conditions by setting specific elements to zero.\nnp.diag: Combines the diagonals to form the matrix A."
  },
  {
    "objectID": "posts/HW4/index.html#advancing-time-with-matrix-multiplication",
    "href": "posts/HW4/index.html#advancing-time-with-matrix-multiplication",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Advancing Time with Matrix Multiplication",
    "text": "Advancing Time with Matrix Multiplication\nNext, we define advance_time_matvecmul to apply A to our grid u:\n\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThe advance_time_matvecmulfunction returns the updated heat distribution grid after one timestep. - A @ u.flatten(): Multiplies the matrix A with the flattened version of u, treating the 2D grid as a 1D vector. - .reshape((N, N)): Reshapes the result back to a 2D array after matrix multiplication. - u = u + epsilon * (...): Updates the grid state based on the finite difference equation, with epsilon controlling the diffusion rate."
  },
  {
    "objectID": "posts/HW4/index.html#running-the-simulation-and-visualizing",
    "href": "posts/HW4/index.html#running-the-simulation-and-visualizing",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Running the Simulation and Visualizing",
    "text": "Running the Simulation and Visualizing\nNow lets run 2700 iterations and store snapshots every 300 steps for visualization.\n\nimport time\n\n# set up \nA = get_A(N)\nu = u0.copy()\niterations = 2700\nsnapshots = []\n\nstart_time = time.time()\nfor k in range(1, iterations + 1): # loop 2700 times\n    u = advance_time_matvecmul(A, u, epsilon)\n    # record every 300 iterations\n    if k % 300 == 0:\n        snapshots.append(u.copy())  # Store snapshots in separate array for later visualization\nend_time = time.time()\n\nmatrix_mult_time = end_time - start_time\n\nprint(f\"Simulation took {matrix_mult_time:.2f} seconds.\")\n\nSimulation took 159.42 seconds.\n\n\n\n# plot iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nfor i, ax in enumerate(axes.flat):\n    im = ax.imshow(snapshots[i], cmap='viridis')\n    ax.set_title(f\"Iteration {(i + 1) * 300}\")\n    fig.colorbar(im, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nIt took a while, but our heat diffusion with matrix multiplication looks pretty nice!"
  },
  {
    "objectID": "posts/HW4/index.html#converting-to-sparse-format",
    "href": "posts/HW4/index.html#converting-to-sparse-format",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Converting to Sparse Format",
    "text": "Converting to Sparse Format\nI’ll define a function get_sparse_A that converst matrix A into sparse format.\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    Constructs the sparse 2D finite difference matrix A for simulating 2D heat diffusion\n    on an N x N grid using sparse matrix format in JAX.\n\n    Args:\n        N (int): Size of one dimension of the N x N grid.\n\n    Returns:\n        A (jax.experimental.sparse.BCOO): 2D finite difference sparse matrix of size N^2 x N^2.\n    \"\"\"\n    n = N * N  # Total number of grid points\n\n    # Diagonals\n    main_diag = -4 * jnp.ones(n)\n    side_diag = jnp.ones(n - 1)\n    up_down_diag = jnp.ones(n - N)\n\n    # Use `.at[]` to set boundary entries for the side diagonals to zero\n    side_diag = side_diag.at[N-1::N].set(0)\n\n    # Stack indices and data for BCOO format\n    row_indices = []\n    col_indices = []\n    data = []\n\n    # Main diagonal\n    row_indices.append(jnp.arange(n))\n    col_indices.append(jnp.arange(n))\n    data.append(main_diag)\n\n    # Side diagonals (left and right)\n    row_indices.append(jnp.arange(n - 1))\n    col_indices.append(jnp.arange(1, n))\n    data.append(side_diag)\n\n    row_indices.append(jnp.arange(1, n))\n    col_indices.append(jnp.arange(n - 1))\n    data.append(side_diag)\n\n    # Up and down diagonals (top and bottom)\n    row_indices.append(jnp.arange(n - N))\n    col_indices.append(jnp.arange(N, n))\n    data.append(up_down_diag)\n\n    row_indices.append(jnp.arange(N, n))\n    col_indices.append(jnp.arange(n - N))\n    data.append(up_down_diag)\n\n    # Concatenate all indices and data\n    row_indices = jnp.concatenate(row_indices)\n    col_indices = jnp.concatenate(col_indices)\n    data = jnp.concatenate(data)\n\n    indices = jnp.stack([row_indices, col_indices], axis=1)\n\n    # Create the sparse BCOO matrix\n    A = sparse.BCOO((data, indices), shape=(n, n))\n    return A\n\n\n\nThe get_sparse_A function - n = N * N: Computes the total number of grid points. - Sets the diagonals. - side_diag = side_diag.at[N-1::N].set(0) sets elements in side_diag at every N-th position to zero. - Initializes lists to hold row indices, column indices, and values for each non-zero entry in the matrix A. - Fills the diagonals (main, side, up and down) - After constructing each part of the matrix, concatenates all row indices, column indices, and data values. - A = sparse.BCOO((data, indices), shape=(n, n)): Creates a sparse matrix in BCOO format."
  },
  {
    "objectID": "posts/HW4/index.html#running-the-simulation-and-visualizing-1",
    "href": "posts/HW4/index.html#running-the-simulation-and-visualizing-1",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Running the Simulation and Visualizing",
    "text": "Running the Simulation and Visualizing\nLet’s run our simulaton for 2700 iterations, taking snapshots every 300 steps.\nWe’ll also use the jit-ed version of the advance_time_matvecmul function.\n\nA = get_sparse_A(N)\nu = u0.copy()\niterations = 2700\nsnapshots = []\n\n# Apply JIT to advance_time_matvecmul\nfrom jax import jit\nadvance_time_matvecmul_jit = jit(advance_time_matvecmul)\n\n\nstart_time = time.time()\nfor k in range(1, iterations + 1):\n    u = advance_time_matvecmul_jit(A, u, epsilon)\n    # record every 300 iterations\n    if k % 300 == 0:\n        snapshots.append(u.copy())  # Store snapshots in separate array for later visualization\nend_time = time.time()\n\nsparse_time = end_time - start_time\n\nprint(f\"Simulation took {sparse_time:.2f} seconds.\")\n\nSimulation took 1.05 seconds.\n\n\n\n# plot iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nfor i, ax in enumerate(axes.flat):\n    im = ax.imshow(snapshots[i], cmap='viridis')\n    ax.set_title(f\"Iteration {(i + 1) * 300}\")\n    fig.colorbar(im, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nWow! We got the same result, but it was WAY faster than before. Let’s try yet another approach."
  },
  {
    "objectID": "posts/HW4/index.html#advancing-with-array-operations",
    "href": "posts/HW4/index.html#advancing-with-array-operations",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Advancing with Array Operations",
    "text": "Advancing with Array Operations\nI’ve defined another function advance_time_numpy which uses np.roll().\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the heat simulation by one timestep using direct numpy operations.\n    \n    Args:\n        u (numpy.ndarray): N x N array representing the grid state at timestep k.\n        epsilon (float): Stability constant.\n    \n    Returns:\n        numpy.ndarray: N x N array representing the grid state at timestep k+1.\n    \"\"\"\n    # Pad the array with zeros on all sides\n    u_padded = np.pad(u, ((1, 1), (1, 1)), mode='constant', constant_values=0)\n    \n    # Apply the heat equation using np.roll to shift the grid\n    u_next = u + epsilon * (\n        np.roll(u_padded, shift=1, axis=0)[1:-1, 1:-1] +  # up\n        np.roll(u_padded, shift=-1, axis=0)[1:-1, 1:-1] + # down\n        np.roll(u_padded, shift=1, axis=1)[1:-1, 1:-1] +  # left\n        np.roll(u_padded, shift=-1, axis=1)[1:-1, 1:-1] - # right\n        4 * u  # center\n    )\n    \n    return u_next\n\n\n\n\nnp.pad: Adds a zero border around the grid, simulating the boundary condition.\nnp.roll: Shifts the grid in each direction (up, down, left, right) to compute neighbors.\nu + epsilon * (...): Updates the heat grid based on neighboring values without using a large matrix A."
  },
  {
    "objectID": "posts/HW4/index.html#running-the-simulation-and-visualizing-2",
    "href": "posts/HW4/index.html#running-the-simulation-and-visualizing-2",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Running the Simulation and Visualizing",
    "text": "Running the Simulation and Visualizing\nAs before we’ll do 2700 iterations with snapshots every 300 steps.\n\nu = u0.copy()\niterations = 2700\nsnapshots = []\n\nstart_time = time.time()\nfor k in range(1, iterations + 1):\n    u = advance_time_numpy(u, epsilon)\n    if k % 300 == 0:\n        snapshots.append(u.copy())  # Store snapshots in separate array for later visualization\nend_time = time.time()\n\nnumpy_time = end_time - start_time\n\nprint(f\"Simulation took {numpy_time:.2f} seconds.\")\n\nSimulation took 0.40 seconds.\n\n\n\n# plot iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nfor i, ax in enumerate(axes.flat):\n    im = ax.imshow(snapshots[i], cmap='viridis')\n    ax.set_title(f\"Iteration {(i + 1) * 300}\")\n    fig.colorbar(im, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nHoly smokes! It’s even faster than before (about 10x than sparse matrix and 100x faster than matrix multiplication)."
  },
  {
    "objectID": "posts/HW4/index.html#running-the-simulation-and-visualizing-3",
    "href": "posts/HW4/index.html#running-the-simulation-and-visualizing-3",
    "title": "Simulating Heat Diffusion in 2D",
    "section": "Running the Simulation and Visualizing",
    "text": "Running the Simulation and Visualizing\nLet’s run our 2700 iterations and see how it compares to the previous approaches.\n\nu = u0.copy()\niterations = 2700\nsnapshots = []\n\nstart_time = time.time()\nfor k in range(1, iterations + 1):\n    u = advance_time_jax(u, epsilon)\n    if k % 300 == 0:\n        snapshots.append(u.copy())  # Store snapshots in separate array for later visualization\nend_time = time.time()\n\njax_time = end_time - start_time\n\nprint(f\"Simulation took {jax_time:.2f} seconds.\")\n\nSimulation took 0.20 seconds.\n\n\n\n# plot iterations\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nfor i, ax in enumerate(axes.flat):\n    im = ax.imshow(snapshots[i], cmap='viridis')\n    ax.set_title(f\"Iteration {(i + 1) * 300}\")\n    fig.colorbar(im, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nWow!! This was the fastest yet. Let’s compare all the methods now."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Recommending Movies with Web-scraped Data",
    "section": "",
    "text": "Today I’ll be using webscraping (scrapy) to answer the following question: What movie or TV shows share actors with your favorite movie or show?\nFirst, I’ll write a webscraper for finding shared actors on https://www.themoviedb.org/. Then, I’ll use the results from my scraper to make recommendations.\n\n1. Setup\nFirst, I need to get the url of the movie I want recommendations from. My favorite movie is The Lord of the Rings: The Return of the King (2003), so I’ll be using this link:\nhttps://www.themoviedb.org/movie/122-the-lord-of-the-rings-the-return-of-the-king\nNext, I’ll initialize project, running the following in the terminal:\nconda activate PIC16B-24F\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis creates a scrapy project called TMDB_scraper’. From here, I need to go into my settings.py file and add\nCLOSESPIDER_PAGECOUNT = 20\nThis is just for testing (it prevents your scraper from downloading too much data) and we’ll remove it later. With this, we can move onto writing our scraper.\n\n\n2. Write the Scraper\nIn a new file called tmdb_spider.py inside the spiders directory, I’ll define my scraper function.\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nclass TmdbSpider(scrapy.Spider) defines a new class called TmdbSpider that inherits from Scrapy’s base Spider class. The spider is given a name 'tmdb_spider. __init__ initializes the class, taking in a subdir argument and two other optional *args and **kwargs.\nself.start_urls is a list of URLs that the spider will start scraping from. Here, it’s defined as [f”https://www.themoviedb.org/movie/{subdir}/”], where {subdir} is replaced with the value passed to the spider.\nNext, I’ll define three parsing methods for the TmdbSpider class.\n\ndef parse(self,response):\n    #redirect to cast and crew page, call parse_full_credits on response object \n    cast_url =  response.url + \"/cast\"\n    yield scrapy.Request(response.urljoin(cast_url), callback=self.parse_full_credits)\n\nThe first method, parse(self,response) first sets the url of the cast and crew page by adding “/cast” to the url of the response object (in our case, https://www.themoviedb.org/movie/122-the-lord-of-the-rings-the-return-of-the-king).\nNext using scrapy.Request, its make a request to this url (response.urljoin() ensures it is a url), and uses the callback= function to call the parse_full_credits so that when the request to the cast page is completed, parse_full_credits will be called with the response of that page.\n\ndef parse_full_credits(self, response):\n    # Select all actor (no crew) links\n    actor_links = response.css(\"ol.people.credits:not(.crew) li div.info a::attr(href)\").getall()\n    # create request for each actor link, calling parse_actor_page on it \n    for link in actor_links:\n        yield scrapy.Request(response.urljoin(link), callback=self.parse_actor_page)\n\nThe next method parse_full_credits finds the links to each actor’s page and will then makes requests to each of those pages.\nUsing response.css, it collects all the links for the actors (excluding crew). Then with a simple for loop, it goes through each link, sending a request to that actor’s page and calling parse_actor_page on the response of that page.\n\ndef parse_actor_page(self, response):\n    # get actor name\n    actor_name = response.css(\"h2.title a::text\").get()\n\n    # Find all titles in the acting section\n    titles = response.css(\"h3:contains('Acting') + table td.role.true.account_adult_false.item_adult_false a.tooltip bdi::text\").getall()\n    # remove duplicates to ensure all titles are unique\n    unique_titles = list(set(titles))  \n\n    # Yield dictionary the actor's name and each title\n    for title in unique_titles:\n        yield {\"actor\": actor_name, \"movie_or_TV_name\": title}\n\nThe last method parse_actor_page goes through each actor’s page and retrieves the actor’s name and a list of unique movie or TV titles in which they have performed, found in their “Acting” section.\nFirst, it retrieves the actor name using response.css and the appropriate css selector.\nThen, it selects all the titles in the acting section, once again using response.css and itscss selector.\nTo remove duplicates, it runs list(set(titles))\nFinally, it runs through each title and yields a dictionary with “actor” as the actor name for that page, and “movie_or_TV_name” as the title in the for loop.\nAnd that’s our scraper! To test it, I use the scrapy shell. For instance in the terminal:\nscrapy shell “https://www.themoviedb.org/movie/122-the-lord-of-the-rings-the-return-of-the-king” actor_links = response.css(“ol.people.credits:not(.crew) li div.info a::attr(href)”).getall() print(actor_links)\nWith this I can check that my actor_links inside parse_full_credits is properly selecting all the actors under the Cast section (not crew).\nAfter extensive testing with the scrapy shell and I’m sure my scraper is working, I can use it to extract the data I want.\nIn the settings.py file, I’ll comment out\nCLOSESPIDER_PAGECOUNT = 20\nand in the terminal, run\nscrapy crawl tmdb_spider -o results.csv -a subdir=122-the-lord-of-the-rings-the-return-of-the-king\nCheking my TMDB folder, there is now a file called results.csv with actors’ names and the movies they starred in!\n\n\n3. Make my Recommendations\n\nimport pandas as pd \nimport plotly\n\nLet’s read in the csv file located in the TMDB file.\n\ndf = pd.read_csv(\"/Users/calebwilliams/TMDB_scraper/results.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nChristian Rivers\nFeeder\n\n\n1\nChristian Rivers\nMinutes Past Midnight\n\n\n2\nChristian Rivers\nThe Lord of the Rings: The Two Towers\n\n\n3\nChristian Rivers\nThe Lord of the Rings: The Return of the King\n\n\n4\nChristian Rivers\nThe Hobbit: An Unexpected Journey\n\n\n5\nChristian Rivers\nThe Hobbit: The Battle of the Five Armies\n\n\n6\nChristian Rivers\nMortal Engines\n\n\n7\nChristian Rivers\nThe Lovely Bones\n\n\n8\nChristian Rivers\nThe Lord of the Rings: The Fellowship of the Ring\n\n\n9\nChristian Rivers\nThe Making of 'The Frighteners'\n\n\n\n\n\n\n\nIn order to make a recommendation, I ant to make a sorted list with the top movies and TV shows that share actors with my favorite movie.\nI’ll use .value_counts() to get the counts of actors for each movie.\n\nrecommmended_titles = df['movie_or_TV_name'].value_counts().reset_index()\nrecommmended_titles.columns = ['movie_name', 'number_of_shared_actors']\n# 1st movie is my favorite movie, so I don't want to recommend that\nrecommmended_titles = recommmended_titles.drop(0) \n\n# top results \nrecommmended_titles.head(25)\n\n\n\n\n\n\n\n\nmovie_name\nnumber_of_shared_actors\n\n\n\n\n1\nThe Lord of the Rings: The Two Towers\n46\n\n\n2\nThe Lord of the Rings: The Fellowship of the Ring\n42\n\n\n3\nA Filmmaker's Journey: Making 'The Return of t...\n20\n\n\n4\nThe Hobbit: An Unexpected Journey\n19\n\n\n5\nHercules: The Legendary Journeys\n17\n\n\n6\nThe Hobbit: The Battle of the Five Armies\n16\n\n\n7\nA Passage to Middle-Earth: Making of 'Lord of ...\n16\n\n\n8\nFilm Collectibles: Capturing Movie Memories\n15\n\n\n9\nQuest for the Ring\n15\n\n\n10\nReunited Apart\n15\n\n\n11\nRingers: Lord of the Fans\n14\n\n\n12\nThe Hobbit: The Desolation of Smaug\n13\n\n\n13\nXena: Warrior Princess\n13\n\n\n14\nThe Making of The Fellowship of the Ring\n13\n\n\n15\nBeyond the Movie: The Fellowship of the Ring\n12\n\n\n16\nThe Quest Fulfilled: A Director's Vision\n11\n\n\n17\nLegend of the Seeker\n11\n\n\n18\nKing Kong\n11\n\n\n19\nBeyond the Movie: The Return of the King\n10\n\n\n20\nMortal Engines\n10\n\n\n21\nThe Late Show with Stephen Colbert\n9\n\n\n22\nPower Rangers\n8\n\n\n23\nThe Brokenwood Mysteries\n7\n\n\n24\nFrodo Is Great... Who Is That?!!\n6\n\n\n25\nThe Frighteners\n6\n\n\n\n\n\n\n\nVoila! I now have a list of movies that are sorted by number of shared actors. Based on this list, the number one recommendation for me is The Lord of the Rings: The Two Towers (I’ve already seen it!).\nLet’s go one step further and make a simple bar chart using plotly.\n\nimport plotly.express as px\n\n# get top titles\ntop_recommended_titles = recommmended_titles.head(25)\n\nfig = px.bar(top_recommended_titles, \n             x='number_of_shared_actors', \n             y='movie_name', \n             orientation='h', # horiztonal orientation\n             title='Top Movies and TV Shows That Share Actors with Your Favorite Movie',\n             labels={'number_of_shared_actors': 'Number of Shared Actors', 'movie_name': 'Movie or TV Show'})\n\nfig.update_layout(yaxis={'categoryorder':'total ascending'}) # set order from high to low\nfig.update_traces(marker_color='orange')\n\nfig.show()"
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "Building a Neural Network for Cat & Dog Image Classification",
    "section": "",
    "text": "Hello, today we’ll tackle the challenge of teaching a machine learning model to classify images of cats and dogs using TensorFlow and Keras. We’ll explore the following concepts:\n\nLoading and preprocessing data with TensorFlow Datasets.\nBuilding and training deep learning models.\nUsing data augmentation to improve model robustness.\nExploring transfer learning to achieve high accuracy efficiently.\n\nBy the end of this tutorial, we’ll demonstrate how to achieve a validation accuracy of over 93% using transfer learning!\n\n1. Load Packages and Obtain Data\nFirst, we’ll load the required libraries:\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\nWe’ll use the cats_vs_dogs dataset from TensorFlow Datasets, which contains labeled images of cats and dogs. We’ll split it into training, validation, and testing datasets, reserving 40% for training and 10% each for validation and testing.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nSince the dataset contains images of different sizes, we’ll resize them to a fixed size of 150x150.\n\n\n2. Data Preprocessing\n\nResizing Images\nThe dataset contains images of varying sizes. To standardize input, we’ll resize all images to 150x150 pixels:\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\n\nOptimizing Data Loading\nWe’ll batch the dataset into groups of 64 images and use prefetching to load data efficiently:\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\n\nVisualizing the Dataset\nTo ensure the dataset is loaded correctly, let’s visualize a few examples. We’ll write a function to display three images of cats and three images of dogs:\n\ndef visualize_cats_and_dogs(dataset):\n    # set 2x3 subplot\n    fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n    cats = 0\n    dogs = 0\n    for images, labels in dataset.unbatch():\n        if labels == 0 and cats &lt; 3:  # If Label 0: Cat\n            axes[0, cats].imshow(images.numpy().astype(\"uint8\"))\n            axes[0, cats].axis(\"off\")\n            cats += 1\n        elif labels == 1 and dogs &lt; 3: # If Label 1: Dog\n            axes[1, dogs].imshow(images.numpy().astype(\"uint8\"))\n            axes[1, dogs].axis(\"off\")\n            dogs += 1\n\n        # Stop when 3 cats and 3 dogs are plotted\n        if cats == 3 and dogs == 3:\n            break\n\n    plt.tight_layout()\n    plt.show()\n\n\n# Call the function to visualize\nvisualize_cats_and_dogs(train_ds)\n\n\n\n\n\n\n\n\nSo cute!!\n\n\nCheck Label Frequencies and Calculate Baseline Accuracy\nTo evaluate our model, we first need to calculate the baseline accuracy. The baseline is the accuracy of always predicting the most frequent class.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n# Count occurrences of each label\nlabel_counts = {0: 0, 1: 0}  # 0: cat, 1: dog\n\nfor label in labels_iterator:\n    label_counts[label] += 1\n\nprint(f\"Cats: {label_counts[0]} images\")\nprint(f\"Dogs: {label_counts[1]} images\")\n\nCats: 4637 images\nDogs: 4668 images\n\n\n\ntotal_images = label_counts[0] + label_counts[1]\nmost_frequent_label_count = max(label_counts.values())\nbaseline_accuracy = most_frequent_label_count / total_images\n\nprint(f\"Baseline accuracy: {baseline_accuracy:.2%}\")\n\nBaseline accuracy: 50.17%\n\n\nSo our baseline accuracy is 50.17%. We need to do better than this in order to have created a good model! (If not, our model’s predictions are no better than random guessing!)\n\n\n\n3. First Model\nOur first model is a convolutional neural network (CNN) with the following layers:\n\nTwo Conv2D layers for feature extraction.\nTwo MaxPooling2D layers for downsampling.\nA Flatten layer to prepare data for dense layers.\nA Dense layer for classification.\nA Dropout layer for regularization.\n\nBefore training, we compile the model with the following configurations: - Optimizer: We use the Adam optimizer, a popular choice for training deep learning models due to its efficiency and adaptive learning rates. - Loss Function: The binary cross-entropy loss function is used because we are solving a binary classification problem (cats or dogs) - Metrics: Used to track the model’s accuracy during training and validation.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n\n# Define the model\nmodel1 = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)), # First convolutional layer to detect features\n    MaxPooling2D(pool_size=(2, 2)), # Reduces size of the feature map\n    Conv2D(64, (3, 3), activation='relu'), # Second convolutional layer for more complex features\n    MaxPooling2D(pool_size=(2, 2)), # Further reduces feature map size\n    Flatten(), # Flattens the 2D data into 1D for the next layers\n    Dense(128, activation='relu'), # Fully connected layer with 128 neurons\n    Dropout(0.5), # Drops 50% of neurons randomly to prevent overfitting\n    Dense(1, activation='sigmoid')  # Output layer with one neuron for binary classification\n])\n\n# Compile the model\nmodel1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy']) # track accuracy during compilation\n\n# Train the model\nhistory = model1.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 19s 84ms/step - accuracy: 0.5490 - loss: 58.3230 - val_accuracy: 0.5206 - val_loss: 0.6913\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.6006 - loss: 0.6581 - val_accuracy: 0.5748 - val_loss: 0.6807\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.6909 - loss: 0.5682 - val_accuracy: 0.5739 - val_loss: 0.7301\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.7471 - loss: 0.4790 - val_accuracy: 0.6071 - val_loss: 0.8223\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.7979 - loss: 0.4159 - val_accuracy: 0.6032 - val_loss: 0.8988\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.8224 - loss: 0.3812 - val_accuracy: 0.6066 - val_loss: 0.9950\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.8600 - loss: 0.3242 - val_accuracy: 0.5980 - val_loss: 0.9706\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.8758 - loss: 0.2910 - val_accuracy: 0.6109 - val_loss: 1.3110\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9105 - loss: 0.2286 - val_accuracy: 0.6122 - val_loss: 1.5633\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.9250 - loss: 0.1893 - val_accuracy: 0.6075 - val_loss: 1.6101\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9400 - loss: 0.1503 - val_accuracy: 0.6191 - val_loss: 1.4108\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9514 - loss: 0.1440 - val_accuracy: 0.6204 - val_loss: 1.5965\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9575 - loss: 0.1263 - val_accuracy: 0.6247 - val_loss: 1.7014\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9643 - loss: 0.1024 - val_accuracy: 0.6126 - val_loss: 1.8296\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.9688 - loss: 0.0940 - val_accuracy: 0.6045 - val_loss: 1.5185\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.9751 - loss: 0.0789 - val_accuracy: 0.6298 - val_loss: 1.7133\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9775 - loss: 0.0751 - val_accuracy: 0.6264 - val_loss: 1.8337\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.9775 - loss: 0.0620 - val_accuracy: 0.6328 - val_loss: 2.0004\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 32ms/step - accuracy: 0.9789 - loss: 0.0689 - val_accuracy: 0.6303 - val_loss: 1.9404\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 31ms/step - accuracy: 0.9791 - loss: 0.0786 - val_accuracy: 0.6255 - val_loss: 1.6583\n\n\nLet’s plot our training and validation accuracies for easier understanding:\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nNice! We can see that the traing accuracy is much separated from the validation accuaracy, settling at around 98%, with the validation accuracy settling at around 60%.\nTo get to this model I tried the following:\n\nAdjusted the number of filters in Conv2D layers (from 64 to 32) to reduce overfitting.\nDecreased the dropout rate from 0.7 to 0.5 to deal with underfitting.\n\n\nThe validation accuracy of model1 stabilized between 60% and 63% during training.\nThis is significantly better than the baseline accuracy of 50.17% (which assumes the model always guesses the majority class).\nThe training accuracy reached around 98% whiile the validation accuracy reached only around 65%. This is a clear sign of overfitting as the model is learning to fit the training data extremely well but struggles to generalize to unseen validation data.\n\nNot bad, but we can do better!\n\n\n4. Model with Data Augmentation\nNow let’s try data augmentation to enhance our training data. Augmentation generates modified versions of existing images, such as flipped or rotated images, which helps the model generalize better to unseen data. We’ll use RandomFlip and RandomRotation layers to augment the data.\nFirst let’s visualize how RandomFlip alters an image:\n\nfrom tensorflow.keras.layers import RandomFlip, RandomRotation\n\n\nrandom_flip = RandomFlip(\"horizontal_and_vertical\")\n\n# pull example image from  dataset\nfor image, label in train_ds.take(1):\n    original_image = image[0].numpy().astype(\"uint8\")\n    break\n\nplt.figure(figsize=(12, 6))\n\n# Original image\nplt.subplot(1, 4, 1)\nplt.imshow(original_image)\nplt.title(\"Original\")\nplt.axis(\"off\")\n\n# Generate flipped images\nfor i in range(2, 5):\n    flipped_image = random_flip(original_image[np.newaxis, ...]).numpy()[0].astype(\"uint8\")\n    plt.subplot(1, 4, i)\n    plt.imshow(flipped_image)\n    plt.title(f\"Flipped {i-1}\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nHow about RandomRotation?\n\nrandom_rotation = RandomRotation(0.2) # rotate 20 degrees\n\nplt.figure(figsize=(12, 6))\n\n# Original image\nplt.subplot(1, 4, 1)\nplt.imshow(original_image)\nplt.title(\"Original\")\nplt.axis(\"off\")\n\n# Generate rotated images\nfor i in range(2, 5):\n    rotated_image = random_rotation(original_image[np.newaxis, ...]).numpy()[0].astype(\"uint8\")\n    plt.subplot(1, 4, i)\n    plt.imshow(rotated_image)\n    plt.title(f\"Rotated {i-1}\")\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nOkay so now we’ve seen how flipping and rotating an image works, let’s build a new model incorporating the data augmentation layers. We’ll add RandomFlip and RandomRotation as the first layers of the model to ensure every input image is augmented. The remaining layers are similar to a standard convolutional neural network (CNN).\n\nmodel2 = Sequential([\n    # Data augmentation layers\n    RandomFlip(\"horizontal_and_vertical\"),\n    RandomRotation(0.2),\n\n    # Convolutional and pooling layers, same as model1\n    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory2 = model2.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 13s 49ms/step - accuracy: 0.5349 - loss: 68.1902 - val_accuracy: 0.5559 - val_loss: 0.6837\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 45ms/step - accuracy: 0.5243 - loss: 0.6930 - val_accuracy: 0.5426 - val_loss: 0.6841\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 45ms/step - accuracy: 0.5316 - loss: 0.6899 - val_accuracy: 0.5469 - val_loss: 0.6804\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 45ms/step - accuracy: 0.5519 - loss: 0.6841 - val_accuracy: 0.5843 - val_loss: 0.6783\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.5544 - loss: 0.6845 - val_accuracy: 0.5993 - val_loss: 0.6651\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.5874 - loss: 0.6759 - val_accuracy: 0.6032 - val_loss: 0.6646\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.5899 - loss: 0.6732 - val_accuracy: 0.6118 - val_loss: 0.6599\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 46ms/step - accuracy: 0.5958 - loss: 0.6753 - val_accuracy: 0.6187 - val_loss: 0.6541\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 45ms/step - accuracy: 0.5879 - loss: 0.6754 - val_accuracy: 0.6152 - val_loss: 0.6565\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.5966 - loss: 0.6665 - val_accuracy: 0.6178 - val_loss: 0.6604\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6149 - loss: 0.6607 - val_accuracy: 0.6028 - val_loss: 0.6613\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 45ms/step - accuracy: 0.6031 - loss: 0.6674 - val_accuracy: 0.6161 - val_loss: 0.6587\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step - accuracy: 0.5590 - loss: 0.6823 - val_accuracy: 0.6294 - val_loss: 0.6523\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6255 - loss: 0.6593 - val_accuracy: 0.6410 - val_loss: 0.6449\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step - accuracy: 0.6282 - loss: 0.6569 - val_accuracy: 0.6384 - val_loss: 0.6435\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6314 - loss: 0.6539 - val_accuracy: 0.6225 - val_loss: 0.6508\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step - accuracy: 0.6069 - loss: 0.6642 - val_accuracy: 0.6337 - val_loss: 0.6413\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6202 - loss: 0.6539 - val_accuracy: 0.6561 - val_loss: 0.6310\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 45ms/step - accuracy: 0.6320 - loss: 0.6515 - val_accuracy: 0.6397 - val_loss: 0.6435\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6314 - loss: 0.6494 - val_accuracy: 0.6247 - val_loss: 0.6496\n\n\nLike before, let’s visualize our model’s training and testing accuracies:\n\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe validation accuracy of model2 stabilized between 62% and 64% during training.\nCompared to model1 which stabilized at around 60 to 63%, this is slightly better.\nThe training accuracy reached around 63%, while while the validation accuracy also reached around 63%. Compared to model1, which achieved a much higher training accuracy of 98% and showed clear signs of overfitting, model2 demonstrates significantly less overfitting. This suggests that the addition of data augmentation helped the model generalize better to unseen data.\n\nCompared to model1, this is definitely an improvement! Let’s take it a step further.\n\n\n5. Data Preprocessing\nFor our next model, let’s start by normalizing the pixel values in our images. Neural networks often perform better when input values fall within a consistent range. Here, we’ll rescale pixel values from [0, 255] to [-1, 1] using the Rescaling layer:\n\n# define preprocessing layer\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\nWe’ll integrate this preprocessing step into our new model, Model 3:\n\nmodel3 = Sequential([\n    # Preprocessing layer to normalize inputs\n    preprocessor,\n\n    # Data augmentation layers\n    RandomFlip(\"horizontal_and_vertical\"),\n    RandomRotation(0.2),\n\n    # Convolutional and pooling layers\n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),  # add an additional convolution layer for increased accuracy\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.3), # decreased dropout for higher accuracy\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel3.compile(optimizer='adam',  # start with a smaller learning rate\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory3 = model3.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 27s 135ms/step - accuracy: 0.5372 - loss: 0.7682 - val_accuracy: 0.6406 - val_loss: 0.6333\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 24s 54ms/step - accuracy: 0.6255 - loss: 0.6435 - val_accuracy: 0.7064 - val_loss: 0.5688\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.6736 - loss: 0.6031 - val_accuracy: 0.7137 - val_loss: 0.5514\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.6982 - loss: 0.5778 - val_accuracy: 0.7274 - val_loss: 0.5374\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.7086 - loss: 0.5659 - val_accuracy: 0.7433 - val_loss: 0.5263\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.7204 - loss: 0.5460 - val_accuracy: 0.7489 - val_loss: 0.5188\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.7291 - loss: 0.5341 - val_accuracy: 0.7468 - val_loss: 0.5093\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.7442 - loss: 0.5264 - val_accuracy: 0.7506 - val_loss: 0.5136\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.7503 - loss: 0.5141 - val_accuracy: 0.7498 - val_loss: 0.4986\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.7487 - loss: 0.5106 - val_accuracy: 0.7666 - val_loss: 0.4848\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.7625 - loss: 0.4945 - val_accuracy: 0.7549 - val_loss: 0.5003\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 61ms/step - accuracy: 0.7630 - loss: 0.4924 - val_accuracy: 0.7696 - val_loss: 0.4825\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.7661 - loss: 0.4897 - val_accuracy: 0.7794 - val_loss: 0.4756\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.7743 - loss: 0.4712 - val_accuracy: 0.7730 - val_loss: 0.4776\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.7779 - loss: 0.4732 - val_accuracy: 0.7739 - val_loss: 0.4758\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.7863 - loss: 0.4612 - val_accuracy: 0.7794 - val_loss: 0.4627\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.7854 - loss: 0.4527 - val_accuracy: 0.7889 - val_loss: 0.4506\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.7879 - loss: 0.4475 - val_accuracy: 0.7863 - val_loss: 0.4574\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.7959 - loss: 0.4419 - val_accuracy: 0.8009 - val_loss: 0.4455\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 56ms/step - accuracy: 0.8072 - loss: 0.4304 - val_accuracy: 0.8022 - val_loss: 0.4353\n\n\nAnd of course, visualize our model’s accuracies:\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe validation accuracy of model2 stabilized between 75% and 80% during training.\nCompared to model1 which stabilized at around 60 to 63%, this is much, much better.\nThere does not appaer to be significant overfitting is in model3. The close alignment between training and validation accuracy indicates that the model generalizes well to unseen data.\n\nThis is a huge improvement compared to the previous models! By normalizing pixel values, we’ve managed to significantly improve the validation accuracy, and the close alignment between training and validation accuracy suggests reduced overfitting.\n\n\n6. Transfer Learning\nModel 3 proved to be quite good at predicting cats and dogs. Let’s see if we can create a model that’s near perfect. To do so, We’ll utilize transfer learning by leveraging a pre-trained MobileNetV3 model. Transfer learning allows us to take advantage of a model trained on a vast dataset (e.g., ImageNet) and adapt it to our specific task.\n\n# Load pre-trained MobileNetV3 model\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ━━━━━━━━━━━━━━━━━━━━ 1s 0us/step\n\n\nWe’ll add out new model layer to the base model with data augmentation:\n\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\n\n\nmodel4 = Sequential([\n    # Data augmentation layers\n    RandomFlip(\"horizontal_and_vertical\"),\n    RandomRotation(0.2),\n    base_model_layer,\n    GlobalMaxPooling2D(),\n    # Additional layers\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(2, activation='softmax')\n])\n\n\nmodel4.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory4 = model4.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 26s 101ms/step - accuracy: 0.7770 - loss: 1.1027 - val_accuracy: 0.9523 - val_loss: 0.1306\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 56ms/step - accuracy: 0.8826 - loss: 0.2918 - val_accuracy: 0.9609 - val_loss: 0.1151\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.8946 - loss: 0.2548 - val_accuracy: 0.9600 - val_loss: 0.1107\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9111 - loss: 0.2212 - val_accuracy: 0.9647 - val_loss: 0.0897\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9129 - loss: 0.2188 - val_accuracy: 0.9622 - val_loss: 0.0951\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 57ms/step - accuracy: 0.9217 - loss: 0.1954 - val_accuracy: 0.9678 - val_loss: 0.0845\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 63ms/step - accuracy: 0.9180 - loss: 0.1972 - val_accuracy: 0.9695 - val_loss: 0.0801\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 55ms/step - accuracy: 0.9272 - loss: 0.1819 - val_accuracy: 0.9682 - val_loss: 0.0792\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9240 - loss: 0.1820 - val_accuracy: 0.9708 - val_loss: 0.0750\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.9267 - loss: 0.1744 - val_accuracy: 0.9703 - val_loss: 0.0760\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 62ms/step - accuracy: 0.9345 - loss: 0.1585 - val_accuracy: 0.9708 - val_loss: 0.0810\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 62ms/step - accuracy: 0.9326 - loss: 0.1692 - val_accuracy: 0.9673 - val_loss: 0.0830\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 55ms/step - accuracy: 0.9321 - loss: 0.1675 - val_accuracy: 0.9686 - val_loss: 0.0795\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.9330 - loss: 0.1580 - val_accuracy: 0.9725 - val_loss: 0.0737\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 56ms/step - accuracy: 0.9376 - loss: 0.1543 - val_accuracy: 0.9712 - val_loss: 0.0757\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.9320 - loss: 0.1665 - val_accuracy: 0.9746 - val_loss: 0.0756\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9398 - loss: 0.1461 - val_accuracy: 0.9703 - val_loss: 0.0784\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.9361 - loss: 0.1538 - val_accuracy: 0.9716 - val_loss: 0.0755\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9399 - loss: 0.1537 - val_accuracy: 0.9733 - val_loss: 0.0697\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9370 - loss: 0.1521 - val_accuracy: 0.9699 - val_loss: 0.0705\n\n\n\nmodel4.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip (RandomFlip)             │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation (RandomRotation)     │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_1 (Functional)            │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d                 │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 128)                 │         123,008 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 128)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 64)                  │           8,256 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 3,390,536 (12.93 MB)\n\n\n\n Trainable params: 131,394 (513.26 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\n Optimizer params: 262,790 (1.00 MB)\n\n\n\nAs always, we’ll visualize the model’s accuracies:\n\nplt.plot(history4.history[\"accuracy\"], label = \"training\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nHoly cow (cat?)! This model is by far the most accurate at correctly predicting dogs and cats!\n\nThe validation accuracy of model4 stabilized between 95% and 97% during training.\nCompared to model1 which stabilized at around 60 to 63%, this is significantly better,demonstrating a massive improvement due to transfer learning and a more efficient architecture.\nTraining accuracy also stabilized at 93% to 94%, which is very close to the validation accuracy (95% to 97%), indicating that model4 is not overfitting.\n\nJust by using MobileNetV3, our model outperforms all previous models and is well-generalized with minimal overfitting. This is the power of transfer learning!\n\n\n7. Score on Test Data\nFinally, in order to tell if our model is truly as good at classifying dogs and cats as we think it is, we need to test it on unseen data to verify its ability to generalize beyond the training and validation datasets. We’ll run model 4 on the test_ds dataset we created in step 1.\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - accuracy: 0.9588 - loss: 0.0897\nTest Accuracy: 0.9592\nTest Loss: 0.0959\n\n\nWow! With an accuracy of 95% on the test data set, our transfer learning model really is that good!"
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Web App Message Bank with Dash",
    "section": "",
    "text": "Hello, today we’ll be building a simple web app using Dash that functions as a message bank. Users will be able to submit messages and view a random sample of previously submitted messages. We’ll use Dash by Plotly for the web interface and SQLite to store the messages.\n\n1. Importing Required Packages\nWe’ll start by importing the necessary packages. Dash will be used to build the web interface, while sqlite3 allows us to manage our database.\n\nfrom dash import Dash, html, dash_table, dcc, callback, Output, Input, State\nimport sqlite3\n\n\n\n2. Setting Up the App Layout\nNext, we’ll set up the app layout which defines the structure of our web interface. Here, we set up input fields for messages, buttons for submitting and viewing messages, and we add styling to make the app visually appealing. For extra fanciness, we’ll also load custom fonts from Google Fonts.\n\n# initialize dash app\napp = Dash()\n\n# App layout\napp.layout = html.Div([\n    # load in links to non-default fonts\n    html.Link(href=\"https://fonts.googleapis.com/css2?family=Oswald:wght@200..700&display=swap\", rel=\"stylesheet\"), # Oswald font \n    html.Link( href=\"https://fonts.googleapis.com/css2?family=Oswald:wght@200..700&family=Roboto+Slab:wght@100..900&display=swap\", rel=\"stylesheet\"), # Robot slab font\n    \n    # header \n    html.H1(\"A Simple Message Bank\", \n            style={'text-align': 'center', 'font-family':'Oswald', 'color': '#cf2539', 'text-shadow': '2px 0px 0px black'}),\n    html.Hr(),\n    html.Div([\n        dcc.Input(id='message-input', type='text', placeholder='Enter your message', # input textbox for message \n                  style={'margin': '10px auto', 'display': 'block','font-family':'Roboto Slab'}),\n        dcc.Input(id='handle-input', type='text', placeholder='Your name',  # input textbox for name \n                  style={'margin': '10px auto', 'display': 'block','font-family':'Roboto Slab'}),\n        html.Button('Submit', id='submit-btn', # submit button\n                    style={'margin': '10px auto', 'display': 'block', 'font-family':'Oswald', 'background-color': '#ADD8E6',}),\n        html.Div(id='submission-status'), # divider, where submission status is displayed\n        html.Hr(),\n        # view random messages button\n        html.Button('View Some Messages', id='view-btn', n_clicks=0, \n                    style={'margin': '10px auto', 'display': 'block','font-family':'Oswald', 'background-color': '#ADD8E6',}),\n        html.Div(id='random-messages-display'), # divider, where random messages are displayed\n        html.Hr()\n    ], style={'text-align': 'center'}) # overall centering of buttons/text\n   \n        \n])\n\n\nWe use html.Link to load Google Fonts (Oswald and Roboto Slab) for custom styling (the font embed links are taken from https://fonts.google.com/).\nhtml.H1 defines the title with center alignment, a red color, and shadow effect.\ndcc.Input creates input fields for the message and user’s handle/name with custom font styling and margins.\nhtml.Button elements allow users to submit a message or view messages. Both buttons have customized colors and fonts.\nhtml.Div are where submission status and random messages from the database are displayed\n\n\n\n3. Setting Up the Database\nWe’ll store user-submitted messages and names in a SQLite database. I’ll define a get_message_db function that initializes an SQLite database and creates a table called messages if it doesn’t already exist.\n\ndef get_message_db():\n    global message_db\n    # connect to db\n    message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n    cursor = message_db.cursor()\n    # create messages table with hangle and message cols \n    cursor.execute('CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, handle TEXT, message TEXT)')\n    return message_db\n\nget_message_db: - declares message_db as global so it can be reused within the app. - connects to (or creates) the messages_db.sqlite file. - uses SQL to create a messages table if it doesn’t already exist, with columns for id (primary key), handle, and message.\n\n\n4. Submitting Messages\nNext, we’ll define the `handle_submission function which is triggered when a user clicks the “Submit” button. It inserts a message and handle into the database and provides feedback to the user.\n\n@callback(\n    Output('submission-status', 'children'),\n    Input('submit-btn', 'n_clicks'),\n    [State('message-input', 'value'), State('handle-input', 'value')],\n    prevent_initial_call=True\n)\ndef handle_submission(n_clicks, message, handle):\n    try:\n        if not message or not handle:\n            return html.Div([html.P(\"Error: Please fill in both fields!\")], \n                            style={'margin-bottom': '20px','font-family':'Roboto Slab'})\n        \n        \n        conn = get_message_db()\n        cursor = conn.cursor()\n        cursor.execute('INSERT INTO messages (handle, message) VALUES (?, ?)', (handle, message))\n        conn.commit()\n        conn.close()\n        \n        return html.Div([html.P(f\"Thanks for submitting a message, {handle}!\")], \n                        style={'margin-bottom': '20px','font-family':'Roboto Slab'})\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nFirst we define a @callback function which links the submit-btn button to this function. - Output('submission-status', 'children') updates the submission-status area with feedback. - Input('submit-btn', 'n_clicks') Triggers the callback when submit-btn is clicked. - State reads values from message-input and handle-input fields without directly triggering the function. - prevent_initial_call=True prevents the function from running on page load.\nThe handle_submisison function then: - checks if both the message and handle are filled, and gives an error message if not. - inserts the submitted message and handle into the messages table. - returns a success message once sucessfully inserted into the database\n\n\n5. Retrieving Random Messages\nNow we’ll create a random_messages function selects a specified n number of random messages from the database. For now, I’ll set n to be 5.\n\ndef random_messages(n):\n    conn = sqlite3.connect('messages_db.sqlite')  # connect to database\n    cursor = conn.cursor()\n    # Use parameter substitution to safely inject 'n' into the SQL query\n    cursor.execute('SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?', (n,))\n    n_messages = cursor.fetchall()\n    conn.close()\n    return n_messages\n\nrandom_messages: - Connects to the SQLite database. - Retrieves up to n (5) random rows from the messages table using SQL’s ORDER BY RANDOM() with a limit. - Closes the database connection.\n\n\n6. Displaying Random Messages\nLastly, we’ll define a display_random_messages function that is triggered when the “View Some Messages” button is clicked. It uses the random_messages function to get random messages from the database and displays them.\n\n@callback(\n    Output('random-messages-display', 'children'),\n    Input('view-btn', 'n_clicks'),\n    prevent_initial_call=True\n)\n\ndef display_random_messages(n_clicks):\n        messages = random_messages(5)  # cap of 5 random messages\n        if not messages:  # check if there are messages\n            return html.Div([html.P(\"No messages available.\")], \n                            style={'margin-bottom': '20px','font-family':'Roboto Slab'})\n        \n        return [\n            html.Div([\n                html.P(f\"'{msg}'\", style={'margin-bottom': '0px','font-weight': 'bold'}),   \n                html.P(f\"-{handle}\", style={'margin-top': '0px','font-style': 'italic'})  \n            ], style={'margin-bottom': '20px','font-family':'Roboto Slab'}) \n            for handle, msg in messages\n        ]\n\n@callback connects view-btn to this function. - Output('random-messages-display', 'children') updates the random-messages-display area with message content. - Input('view-btn', 'n_clicks') triggers the function when view-btn is clicked. - prevent_initial_call=True prevents the callback from running on page load.\ndisplay_random_messages calls random_messages(5) to fetch up to 5 random messages. - If no messages are available, it displays a “No messages available” message. - If messages exist, each message is displayed with the message text in bold and the handle in italics.\n\n\n7. Running the App\nFinally, we use launch the app using:\n\nif __name__ == '__main__':\n    app.run_server(port=8052, debug=True)\n\n\n        \n        \n\n\nIt looks pretty good! Lets test the submit and view messages functionality:\n\nHere I submitted a message “Testing testing” with my name, and got a successs message!\nNow let see some messages stored in the database:\n\nAs we can see, the message I added along with others from the database are successfully retrieved and displayed!"
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Classifying Fake News with Keras",
    "section": "",
    "text": "Hello! In today’s blog post, we’ll be building three different machine learning models using Keras to determine whether a news article is fake based on its title, full text, or a combination of both. We’ll compare these models to find out which approach works best for detecting fake news.\n\n1. Acquire Training Data\nFirst, we’ll make sure we’re using the TensorFlow backend and import the necessary libraries.\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\n\nimport numpy as np\nimport pandas as pd\n\nimport re\nimport string\nimport keras\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\nfrom keras import models\n\nimport tensorflow as tf\n\nThen, we’ll load the training data from the provided URL into a pandas DataFrame using pd.read_csv.\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf_train = pd.read_csv(train_url)\n\nLet’s inspect our dataframe:\n\nprint(df_train.shape)\ndf_train.head(5)\n\n(22449, 4)\n\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n2. Make a Dataset\nTo build effective machine learning models, it’s essential to preprocess the text data. We’ll define a function that cleans the text (removing stopwords, punctuation, and converting text to lowercase) in our dataframe and makes a tf.data.Datase. with two inputs, (title, text) and one output, the fake column.\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef make_dataset(df):\n    def clean_text(text):\n        # Make the text Lowercase\n        text = text.lower()\n        # Remove punctuation\n        text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n        # Remove stopwords\n        text = \" \".join([word for word in text.split() if word not in stop_words])\n        return text\n\n    # Apply clean_text function to text and title columns\n    df.loc[:, \"text\"] = df[\"text\"].apply(clean_text)\n    df.loc[:, \"title\"] = df[\"title\"].apply(clean_text)\n\n\n    # Make a tf.data.Dataset\n    dataset = tf.data.Dataset.from_tensor_slices(((df[\"title\"].values, df[\"text\"].values), df[\"fake\"].values))\n    # Batch the dataset\n    dataset = dataset.batch(100)\n    return dataset\n\n# Create tf.data.Dataset\ndataset = make_dataset(df_train)\n\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nLet’s use our function now to create training and testing data sets.\n\n# Split into training and validation datasets\ntrain_size = int(0.8 * len(df_train)) # 80% for training, 20% for testing\ndf_train_data = df_train[:train_size]\ndf_val_data = df_train[train_size:]\n\n# Create tf.data.Dataset for training and validation\ntrain_dataset = make_dataset(df_train_data)\nval_dataset = make_dataset(df_val_data)\n\nIn order to get a sense of our model’s relative performance, we need to establish a base rate. To do so, we’ll count the number of fake news articles and calculate their proportion.\n\n\n\nbase_rate = df_train[\"fake\"].value_counts(normalize=True).max()\nprint(f\"Base rate: {base_rate:.2f}\")\n\nBase rate: 0.52\n\n\nSo we need our model to get an accuracy of at least 52%.\nLet’s also create a Text Vectorization Layer to be used in our first model. We create a TextVectorization layer to convert the text data into a numerical representation that can be fed into a model. This is important because machine learning algorithms work on numbers, not raw text.\n\n#preparing a text vectorization layer for model (Title only)\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,'[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[0])) # only processes the titles\n\n\n\n3. Create Models\n\nModel 1 - Titles Only\nFor our first model we’ll use only the article titles as an input.\nAlong with our Text Vectorization layer we need to make an embedding layer, which is used to convert words from the input text into a numerical format that the model can process.\n\nfrom keras.layers import Embedding, Input, Dense, Dropout, GlobalAveragePooling1D\nfrom keras.models import Model\nfrom keras.utils import plot_model\n\n# Create an embedding layer\nembedding_dim = 10\nembedding_layer = Embedding(input_dim=size_vocabulary, output_dim=embedding_dim)\n\nNow we can define our model using the text vectorization and embedding layers and some dense layers:\n\n# Model 1\ninput_title = Input(shape=(1,), dtype=tf.string, name='title')\n# Vectorize title\nvectorized_title = title_vectorize_layer(input_title)\n# Embed title\nembedded_title = embedding_layer(vectorized_title)\nflattened_title = GlobalAveragePooling1D()(embedded_title)\ndense_output = Dense(16, activation='relu')(flattened_title)\ndropout_layer = Dropout(0.5)(dense_output)\noutput = Dense(1, activation='sigmoid')(dropout_layer)\n\nmodel1 = Model(inputs=input_title, outputs=output, name='model_title')\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nTo ‘fully train’ our model I’ll use EarlyStopping which ensures that the model stops when there is no improvement after a number of epochs, helping to prevent overfitting.\n\n# Train Model 1\nfrom keras.callbacks import EarlyStopping\n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # prevents overfitting by stopping if validation loss doesn't improve for 5 consecutive epochs\n\nhistory1 = model1.fit(\n    train_dataset.map(lambda x, y: (x[0], y)),\n    validation_data=val_dataset.map(lambda x, y: (x[0], y)),\n    epochs=40,\n    callbacks=[early_stopping]\n)\n\nEpoch 1/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.5186 - loss: 0.6930 - val_accuracy: 0.5272 - val_loss: 0.6923\nEpoch 2/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.5124 - loss: 0.6927 - val_accuracy: 0.5272 - val_loss: 0.6908\nEpoch 3/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.5259 - loss: 0.6907 - val_accuracy: 0.5272 - val_loss: 0.6869\nEpoch 4/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.5478 - loss: 0.6876 - val_accuracy: 0.6033 - val_loss: 0.6808\nEpoch 5/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.5806 - loss: 0.6772 - val_accuracy: 0.7207 - val_loss: 0.6555\nEpoch 6/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.6281 - loss: 0.6530 - val_accuracy: 0.6833 - val_loss: 0.6129\nEpoch 7/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.6449 - loss: 0.6204 - val_accuracy: 0.7775 - val_loss: 0.5617\nEpoch 8/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6882 - loss: 0.5741 - val_accuracy: 0.8018 - val_loss: 0.5197\nEpoch 9/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.7413 - loss: 0.5368 - val_accuracy: 0.8089 - val_loss: 0.4800\nEpoch 10/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.7599 - loss: 0.5107 - val_accuracy: 0.8296 - val_loss: 0.4544\nEpoch 11/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.7823 - loss: 0.4787 - val_accuracy: 0.7831 - val_loss: 0.4565\nEpoch 12/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.7862 - loss: 0.4647 - val_accuracy: 0.8497 - val_loss: 0.4001\nEpoch 13/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8070 - loss: 0.4422 - val_accuracy: 0.8619 - val_loss: 0.3740\nEpoch 14/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8132 - loss: 0.4294 - val_accuracy: 0.8628 - val_loss: 0.3670\nEpoch 15/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.8228 - loss: 0.4102 - val_accuracy: 0.8655 - val_loss: 0.3449\nEpoch 16/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.8378 - loss: 0.3912 - val_accuracy: 0.8791 - val_loss: 0.3209\nEpoch 17/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8353 - loss: 0.3902 - val_accuracy: 0.8822 - val_loss: 0.3067\nEpoch 18/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8417 - loss: 0.3768 - val_accuracy: 0.8929 - val_loss: 0.2914\nEpoch 19/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8616 - loss: 0.3477 - val_accuracy: 0.8927 - val_loss: 0.2778\nEpoch 20/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8635 - loss: 0.3426 - val_accuracy: 0.9007 - val_loss: 0.2656\nEpoch 21/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 19ms/step - accuracy: 0.8608 - loss: 0.3335 - val_accuracy: 0.9062 - val_loss: 0.2487\nEpoch 22/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.8543 - loss: 0.3438 - val_accuracy: 0.8904 - val_loss: 0.2772\nEpoch 23/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8677 - loss: 0.3236 - val_accuracy: 0.8989 - val_loss: 0.2523\nEpoch 24/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.8566 - loss: 0.3422 - val_accuracy: 0.9140 - val_loss: 0.2294\nEpoch 25/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8777 - loss: 0.3065 - val_accuracy: 0.8969 - val_loss: 0.2613\nEpoch 26/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.8814 - loss: 0.3002 - val_accuracy: 0.9100 - val_loss: 0.2263\nEpoch 27/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.8678 - loss: 0.3143 - val_accuracy: 0.8973 - val_loss: 0.2400\nEpoch 28/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.8770 - loss: 0.3062 - val_accuracy: 0.8991 - val_loss: 0.2354\nEpoch 29/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8870 - loss: 0.2934 - val_accuracy: 0.9238 - val_loss: 0.2007\nEpoch 30/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8831 - loss: 0.2932 - val_accuracy: 0.9214 - val_loss: 0.1994\nEpoch 31/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.8810 - loss: 0.2957 - val_accuracy: 0.9256 - val_loss: 0.2173\nEpoch 32/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9051 - loss: 0.2533 - val_accuracy: 0.9294 - val_loss: 0.1871\nEpoch 33/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8802 - loss: 0.2978 - val_accuracy: 0.8842 - val_loss: 0.2551\nEpoch 34/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.8978 - loss: 0.2619 - val_accuracy: 0.8679 - val_loss: 0.2813\nEpoch 35/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8956 - loss: 0.2678 - val_accuracy: 0.9296 - val_loss: 0.1925\nEpoch 36/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9080 - loss: 0.2447 - val_accuracy: 0.9249 - val_loss: 0.1895\nEpoch 37/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 18ms/step - accuracy: 0.8880 - loss: 0.2727 - val_accuracy: 0.8840 - val_loss: 0.2566\n\n\nWe’ll plot the traning history and visualize the layers of our model:\n\n# Plot training history\nimport matplotlib.pyplot as plt\n\ndef plot_history(history, model_name):\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title(f'Training and Validation Accuracy for {model_name}')\n    plt.legend()\n    plt.show()\n\nplot_history(history1, \"Model 1 - Title Only\")\n\n\n\n\n\n\n\n\nNot bad! Our model’s training accuracy steadily increases,indicating that the model learns from the data well, and our validation accuracy reached around 90%.\nThere is still a gap between training and validation suggesting some overfitting. Let’s see if we can do better with another model!\n\n# Visualize Model 1\nfrom keras import utils\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel 2 - Text Only\nFor our second model we’ll use only the article text as an input.\nWe’ll need to create a new Text Vectorization layer for text only:\n\n# Preparing a text vectorization layer for the model\n\n\nsize_vocabulary = 2000\n\ndef standardize_text(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation\n\n# Text vectorization layer for text only\ntext_vectorize_layer = TextVectorization(\n    standardize=standardize_text,  # modification from the title vectorization\n    max_tokens=size_vocabulary,  # only consider this many words\n    output_mode='int',\n    output_sequence_length=500\n)\n\ntext_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[1])) # only processes text\n\nLike with model 1 we’ll also create a new embedding layer and define our model using it and the text vectorization layer.\n\n# Create an embedding layer\nembedding_dim = 10\nembedding_layer_text = Embedding(input_dim=size_vocabulary, output_dim=embedding_dim)\n\n# Model 2\ninput_text = Input(shape=(1,), dtype=tf.string, name='text')\nvectorized_text = text_vectorize_layer(input_text)\nembedded_text = embedding_layer_text(vectorized_text)\nflattened_text = GlobalAveragePooling1D()(embedded_text)\ndense_output_text = Dense(16, activation='relu')(flattened_text)\ndropout_layer_text = Dropout(0.5)(dense_output_text)\noutput_text = Dense(1, activation='sigmoid')(dropout_layer_text)\n\nmodel2 = Model(inputs=input_text, outputs=output_text, name='model_text')\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Train Model 2\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # prevents overfitting\n\nhistory2 = model2.fit(\n    train_dataset.map(lambda x, y: (x[1], y)),\n    validation_data=val_dataset.map(lambda x, y: (x[1], y)),\n    epochs=40,\n    callbacks=[early_stopping]\n)\n\nEpoch 1/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 19ms/step - accuracy: 0.5658 - loss: 0.6872 - val_accuracy: 0.9376 - val_loss: 0.6381\nEpoch 2/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.7402 - loss: 0.6027 - val_accuracy: 0.8940 - val_loss: 0.4283\nEpoch 3/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.8574 - loss: 0.4198 - val_accuracy: 0.9570 - val_loss: 0.2580\nEpoch 4/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9118 - loss: 0.2897 - val_accuracy: 0.9619 - val_loss: 0.1902\nEpoch 5/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9310 - loss: 0.2395 - val_accuracy: 0.9648 - val_loss: 0.1581\nEpoch 6/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9357 - loss: 0.2146 - val_accuracy: 0.9666 - val_loss: 0.1521\nEpoch 7/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9446 - loss: 0.1902 - val_accuracy: 0.9677 - val_loss: 0.1317\nEpoch 8/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9489 - loss: 0.1809 - val_accuracy: 0.9697 - val_loss: 0.1231\nEpoch 9/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 24ms/step - accuracy: 0.9533 - loss: 0.1707 - val_accuracy: 0.9735 - val_loss: 0.1153\nEpoch 10/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9571 - loss: 0.1585 - val_accuracy: 0.9708 - val_loss: 0.1120\nEpoch 11/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9566 - loss: 0.1513 - val_accuracy: 0.9530 - val_loss: 0.1202\nEpoch 12/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9585 - loss: 0.1461 - val_accuracy: 0.9744 - val_loss: 0.1025\nEpoch 13/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9611 - loss: 0.1376 - val_accuracy: 0.9733 - val_loss: 0.0993\nEpoch 14/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9649 - loss: 0.1365 - val_accuracy: 0.9737 - val_loss: 0.1032\nEpoch 15/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9651 - loss: 0.1267 - val_accuracy: 0.9751 - val_loss: 0.1002\nEpoch 16/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9605 - loss: 0.1190 - val_accuracy: 0.9753 - val_loss: 0.0969\nEpoch 17/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9670 - loss: 0.1114 - val_accuracy: 0.9748 - val_loss: 0.0910\nEpoch 18/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 17ms/step - accuracy: 0.9692 - loss: 0.1057 - val_accuracy: 0.9739 - val_loss: 0.0905\nEpoch 19/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 19ms/step - accuracy: 0.9719 - loss: 0.1028 - val_accuracy: 0.9731 - val_loss: 0.0941\nEpoch 20/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 18ms/step - accuracy: 0.9704 - loss: 0.0986 - val_accuracy: 0.9757 - val_loss: 0.0898\nEpoch 21/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9746 - loss: 0.0913 - val_accuracy: 0.9759 - val_loss: 0.0910\nEpoch 22/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 25ms/step - accuracy: 0.9732 - loss: 0.0902 - val_accuracy: 0.9766 - val_loss: 0.0894\nEpoch 23/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9764 - loss: 0.0855 - val_accuracy: 0.9753 - val_loss: 0.0902\nEpoch 24/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9764 - loss: 0.0833 - val_accuracy: 0.9753 - val_loss: 0.1039\nEpoch 25/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9804 - loss: 0.0748 - val_accuracy: 0.9775 - val_loss: 0.0967\nEpoch 26/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 18ms/step - accuracy: 0.9805 - loss: 0.0755 - val_accuracy: 0.9773 - val_loss: 0.0869\nEpoch 27/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9789 - loss: 0.0759 - val_accuracy: 0.9768 - val_loss: 0.1012\nEpoch 28/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9827 - loss: 0.0674 - val_accuracy: 0.9793 - val_loss: 0.0884\nEpoch 29/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 18ms/step - accuracy: 0.9813 - loss: 0.0658 - val_accuracy: 0.9773 - val_loss: 0.0968\nEpoch 30/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9807 - loss: 0.0659 - val_accuracy: 0.9793 - val_loss: 0.0955\nEpoch 31/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9837 - loss: 0.0629 - val_accuracy: 0.9800 - val_loss: 0.0869\nEpoch 32/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 31ms/step - accuracy: 0.9825 - loss: 0.0640 - val_accuracy: 0.9786 - val_loss: 0.0975\nEpoch 33/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9834 - loss: 0.0603 - val_accuracy: 0.9788 - val_loss: 0.0932\nEpoch 34/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9793 - loss: 0.0651 - val_accuracy: 0.9797 - val_loss: 0.0970\nEpoch 35/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9869 - loss: 0.0555 - val_accuracy: 0.9784 - val_loss: 0.0987\nEpoch 36/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9869 - loss: 0.0552 - val_accuracy: 0.9784 - val_loss: 0.0978\n\n\n\n# Plot Training History\n\nplot_history(history2, \"Model 2 - Text Only\")\n\n\n\n\n\n\n\n\nWow! Both training and validation accuracy reached around 97-98%! The graph also shows very little fluctuations suggesting that our model generalizes quite well. Still, let’s try one more model and see if we can do even better!\n\n# Visualize Model 2\nutils.plot_model(model2, \"model2.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nModel 3 - Titles and Text\nFinally, for our third model we’ll use both the article titles and text as the inputs.\nLike before, we’ll need a new Text Vectorization leyer, this time that processes both title and text.\n\n# Preparing a shared text vectorization layer for the model (Title and Text)\n\nsize_vocabulary = 2000\n\ndef standardize_both(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation\n\n# Shared text vectorization layer for title and text\nshared_vectorize_layer = TextVectorization(\n    standardize=standardize_both,  # Shared standardization function\n    max_tokens=size_vocabulary,  # only consider this many words\n    output_mode='int',\n    output_sequence_length=500\n)\n\nshared_vectorize_layer.adapt(train_dataset.map(lambda x, y: tf.concat([x[0], x[1]], axis=0))) # processes both title and text\n\nAlso, we’ll need an embedding layer for both title and text:\n\n# Create a shared embedding layer\nembedding_dim = 10\nshared_embedding_layer = Embedding(input_dim=size_vocabulary, output_dim=embedding_dim)\n\n\nfrom keras.layers import Concatenate\n\n# Model 3\ninput_title = Input(shape=(1,), dtype=tf.string, name='title')\ninput_text = Input(shape=(1,), dtype=tf.string, name='text')\n\n# Vectorize title and text inputs\nvectorized_title = shared_vectorize_layer(input_title)\nvectorized_text = shared_vectorize_layer(input_text)\n\n# Embed vectorized title and text\nembedded_title = shared_embedding_layer(vectorized_title)\nembedded_text = shared_embedding_layer(vectorized_text)\n\n# Combine title and text embeddings\ncombined_embeddings = Concatenate(axis=-1)([embedded_title, embedded_text])\nflattened_combined = GlobalAveragePooling1D()(combined_embeddings)\n\ndense_output_combined = Dense(16, activation='relu')(flattened_combined)\ndropout_layer_combined = Dropout(0.5)(dense_output_combined)\noutput_combined = Dense(1, activation='sigmoid')(dropout_layer_combined)\n\nmodel3 = Model(inputs=[input_title, input_text], outputs=output_combined, name='model_combined')\nmodel3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n# Train Model 3\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # prevents overfitting\n\nhistory3 = model3.fit(\n    train_dataset.map(lambda x, y: ({'title': x[0], 'text': x[1]}, y)),\n    validation_data=val_dataset.map(lambda x, y: ({'title': x[0], 'text': x[1]}, y)),\n    epochs=40,\n    callbacks=[early_stopping]\n)\n\nEpoch 1/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 35ms/step - accuracy: 0.5385 - loss: 0.6875 - val_accuracy: 0.8390 - val_loss: 0.6343\nEpoch 2/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 24ms/step - accuracy: 0.7273 - loss: 0.5871 - val_accuracy: 0.9376 - val_loss: 0.3869\nEpoch 3/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 35ms/step - accuracy: 0.8586 - loss: 0.3899 - val_accuracy: 0.9479 - val_loss: 0.2580\nEpoch 4/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9062 - loss: 0.2890 - val_accuracy: 0.9601 - val_loss: 0.1962\nEpoch 5/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 32ms/step - accuracy: 0.9244 - loss: 0.2403 - val_accuracy: 0.9572 - val_loss: 0.1630\nEpoch 6/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 32ms/step - accuracy: 0.9364 - loss: 0.2145 - val_accuracy: 0.9601 - val_loss: 0.1468\nEpoch 7/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 39ms/step - accuracy: 0.9384 - loss: 0.1993 - val_accuracy: 0.9626 - val_loss: 0.1328\nEpoch 8/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 34ms/step - accuracy: 0.9465 - loss: 0.1820 - val_accuracy: 0.9706 - val_loss: 0.1176\nEpoch 9/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 35ms/step - accuracy: 0.9528 - loss: 0.1627 - val_accuracy: 0.9717 - val_loss: 0.1124\nEpoch 10/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 34ms/step - accuracy: 0.9562 - loss: 0.1543 - val_accuracy: 0.9724 - val_loss: 0.1083\nEpoch 11/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 28ms/step - accuracy: 0.9553 - loss: 0.1482 - val_accuracy: 0.9746 - val_loss: 0.1016\nEpoch 12/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 36ms/step - accuracy: 0.9610 - loss: 0.1407 - val_accuracy: 0.9728 - val_loss: 0.1000\nEpoch 13/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 12s 45ms/step - accuracy: 0.9590 - loss: 0.1373 - val_accuracy: 0.9742 - val_loss: 0.0959\nEpoch 14/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9636 - loss: 0.1327 - val_accuracy: 0.9753 - val_loss: 0.0885\nEpoch 15/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9639 - loss: 0.1225 - val_accuracy: 0.9708 - val_loss: 0.0944\nEpoch 16/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 35ms/step - accuracy: 0.9657 - loss: 0.1168 - val_accuracy: 0.9748 - val_loss: 0.0865\nEpoch 17/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 25ms/step - accuracy: 0.9684 - loss: 0.1110 - val_accuracy: 0.9570 - val_loss: 0.1125\nEpoch 18/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9659 - loss: 0.1113 - val_accuracy: 0.9768 - val_loss: 0.0805\nEpoch 19/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 26ms/step - accuracy: 0.9729 - loss: 0.0982 - val_accuracy: 0.9759 - val_loss: 0.0809\nEpoch 20/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 36ms/step - accuracy: 0.9724 - loss: 0.0958 - val_accuracy: 0.9766 - val_loss: 0.0792\nEpoch 21/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 26ms/step - accuracy: 0.9746 - loss: 0.0896 - val_accuracy: 0.9762 - val_loss: 0.0795\nEpoch 22/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 34ms/step - accuracy: 0.9733 - loss: 0.0924 - val_accuracy: 0.9753 - val_loss: 0.0826\nEpoch 23/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 24ms/step - accuracy: 0.9747 - loss: 0.0893 - val_accuracy: 0.9766 - val_loss: 0.0791\nEpoch 24/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 31ms/step - accuracy: 0.9773 - loss: 0.0836 - val_accuracy: 0.9802 - val_loss: 0.0726\nEpoch 25/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9796 - loss: 0.0801 - val_accuracy: 0.9802 - val_loss: 0.0752\nEpoch 26/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 33ms/step - accuracy: 0.9778 - loss: 0.0762 - val_accuracy: 0.9775 - val_loss: 0.0767\nEpoch 27/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9790 - loss: 0.0772 - val_accuracy: 0.9811 - val_loss: 0.0727\nEpoch 28/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9794 - loss: 0.0732 - val_accuracy: 0.9800 - val_loss: 0.0754\nEpoch 29/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9799 - loss: 0.0738 - val_accuracy: 0.9822 - val_loss: 0.0723\nEpoch 30/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 35ms/step - accuracy: 0.9821 - loss: 0.0695 - val_accuracy: 0.9786 - val_loss: 0.0817\nEpoch 31/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 26ms/step - accuracy: 0.9812 - loss: 0.0639 - val_accuracy: 0.9795 - val_loss: 0.0750\nEpoch 32/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 36ms/step - accuracy: 0.9765 - loss: 0.0726 - val_accuracy: 0.9824 - val_loss: 0.0705\nEpoch 33/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 26ms/step - accuracy: 0.9828 - loss: 0.0627 - val_accuracy: 0.9806 - val_loss: 0.0734\nEpoch 34/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 34ms/step - accuracy: 0.9837 - loss: 0.0611 - val_accuracy: 0.9817 - val_loss: 0.0741\nEpoch 35/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 10s 32ms/step - accuracy: 0.9840 - loss: 0.0569 - val_accuracy: 0.9804 - val_loss: 0.0777\nEpoch 36/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9837 - loss: 0.0598 - val_accuracy: 0.9786 - val_loss: 0.0820\nEpoch 37/40\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9823 - loss: 0.0591 - val_accuracy: 0.9784 - val_loss: 0.0816\n\n\n\n# Plot Training History\n\nplot_history(history3, \"Model 3 - Titla and Text\")\n\n\n\n\n\n\n\n\nBeautiful! Our results are almost the same as model 2, with training and validation accuracies reaching 97-98% (though very slightly higher than model 2).The graphs are also very similar with the accuracies quickly plateauing and shwoing good generalization.\n\n# Visualize Model 3\nutils.plot_model(model3, \"model3.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\n\n4. Model Evaluation\nNow we’ll test our best mode’s performance on unseen test data. But which ones is the best model??\nBased on validation accuracy alone, Model 3, which uses both the article title and text as input, slightly outperforms Model 2.\nHowever, the difference in validation accuracy is minimal, and both models show very similar training histories. Given the negligible difference in performance and the added complexity of using two inputs, I would choose Model 2 as the best model due to its simplicity and nearly equivalent performance.\nWe’ll load in the test dataframe:\n\n# Load the test dataframe\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ndf_test = pd.read_csv(test_url)\n\nAnd use the make_dataset function we defined earlier to create a test dataset:\n\n# Create test dataset\ntest_dataset = make_dataset(df_test)\n\nNow let’s see how model 2 performs on the unseen test data:\n\n# Evaluate Model 2\nmodel2_results = model2.evaluate(test_dataset.map(lambda x, y: (x[1], y)))\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9732 - loss: 0.0870\n\n\nNice!! As expected, our Model 2 generalized well to the unseen test data with an accuracy of 97%!\n\n\n5. Embedding Visualization\nFinally, let’s see if we can visualize the embedding that our model learned.\n\nfrom sklearn.decomposition import PCA\n\n\n# Extract embedding weights\nembedding_layer = model2.get_layer('embedding_12')  # Adjust this to match the actual layer name if different\nembedding_weights = embedding_layer.get_weights()[0]\n\n# Use PCA to reduce dimensions to 2D\npca = PCA(n_components=2)\nreduced_embeddings = pca.fit_transform(embedding_weights)\n\n\n# Get vocabulary\nvocab = text_vectorize_layer.get_vocabulary()  # TextVectorization layer used in Model 2\n\n# Select a sample of words to visualize\nwords_to_visualize = 25  # lets choose 25 words\nselected_indices = np.random.choice(len(vocab), words_to_visualize, replace=False)\nselected_words = [vocab[i] for i in selected_indices]\nselected_embeddings = reduced_embeddings[selected_indices]\n\n\n# Plot selected embeddings\nplt.figure(figsize=(12, 8))\nfor word, (x, y) in zip(selected_words, selected_embeddings):\n    plt.scatter(x, y)\n    plt.text(x + 0.01, y + 0.01, word, fontsize=12)\n\nplt.title(\"Word Embeddings Visualization\")\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nVery interesting. Looking at the visualization, we can see some interesting patterns/ associations in the words that the model might have found useful when distinguishing real news from fake news:\n\n“kill” is quite isolated from the others and has high values for PCA component 1. Its positioning could suggest that it is be strongly associated with certain types of content, potentially fake news headlines.\n“representatives” is far to the right along PCA component 1, also positioned away from the cluster of more neutral terms. This word could be associated with political news, which may contain both real and fake news elements.\n“syria” is another isolated word appearing at the bottom left. This word could be heavily related to news topics regarding conflict or international events, and its distance suggests the term is used in unique contexts.\n“november” and “attacking” are two words positioned close to each other, forming a small cluster. This indicates they may frequently appear in similar contexts, perhaps related to news about specific incidents or events during a particular month.\n“fuel” and “authorities” are another pair of words relatively close to one another, suggesting a potential relationship in terms of context—likely related to issues involving government intervention, regulation, or economic matters."
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "Palmer’s Penguins Visualization Tutorial",
    "section": "",
    "text": "Hello, today I am going to be explaining how to construct an interesting data visualization of the Palmer Penguins data set."
  },
  {
    "objectID": "posts/HW0/index.html#scatterplot",
    "href": "posts/HW0/index.html#scatterplot",
    "title": "Palmer’s Penguins Visualization Tutorial",
    "section": "Scatterplot",
    "text": "Scatterplot\nI’ll plot Body Mass vs Flipper length and color the points by Species using the px.scatter function.\n\nfig = px.scatter(data_frame = penguins, \n                 x = \"Flipper Length (mm)\",\n                 y = \"Body Mass (g)\",\n                 color = 'Species'\n                )\nfig.show()\n\n\nWe can see that there does seem to be a positive linear relationship between Flipper Length and Body Mass! Also, it appears that the Gentoo species tend to be much heavier and have longer flippers than Adelie and Chinstrap which are very similar. Let’s make our graph a bit more fancy and show this linear trend on our graph using trendline=\"ols\".\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Flipper Length (mm)\",\n                 y = \"Body Mass (g)\",\n                 color = \"Species\",\n                 trendline=\"ols\",\n                 trendline_scope=\"overall\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 width = 700,\n                 height = 500,\n                 template = 'plotly_white'\n                )\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n\n\nhover_name = \"Species\" makes it so the species name is the title displayed on hover\nhover_data = [\"Island\", \"Sex\"] adds the Island and Sex to be displayed on hover\nwidth and height control the dimensions of the plot\ntemplate = 'plotly_white' makes a simple white theme\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0}) removes unnecessary whitespace\n\nWe can also make the trendline apply for each species by change the trendline_scope= to \"trace\"\n\nfig = px.scatter(data_frame = penguins,\n                 x = \"Flipper Length (mm)\",\n                 y = \"Body Mass (g)\",\n                 color = \"Species\",\n                 trendline=\"ols\",\n                 trendline_scope=\"trace\",\n                 hover_name = \"Species\",\n                 hover_data = [\"Island\", \"Sex\"],\n                 width = 700,\n                 height = 500,\n                 template = 'plotly_white'\n                )\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n\nNice! From this addition we can gather that Gentoo penguins tend to have a greater body mass\nincrease with flipper length increase than Adelie and Chinstrap which have a similar tend."
  },
  {
    "objectID": "posts/HW0/index.html#histogram",
    "href": "posts/HW0/index.html#histogram",
    "title": "Palmer’s Penguins Visualization Tutorial",
    "section": "Histogram",
    "text": "Histogram\nLet’s go one step more and look at just Flipper Length vs Species with a histogram, using the px.histogram function.\n\nfig = px.histogram(penguins,\n                   x = \"Flipper Length (mm)\",\n                   color = \"Species\",\n                   opacity = 0.55,\n                   nbins = 30, \n                   barmode = \"overlay\",\n                   width = 700,\n                   height = 500,\n                   template = 'plotly_white')\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n\nopacity and nbins control how transparent each bin is and the number of bins, respectively\nbarmode selects the format of the histogram; \"overlay\" shows the bars on top of each other, whereas \"group\" shows them with space in between, and stacked shows the counts as a stack of the colors\n\nThis looks good! It shows us what we had learned before from the scatterplot, that the Gentoo have longer flipper lengths on average. It also makes clear that the average flipper length for Chinstraps is slightly more than that of Adelie, which was hard to see before with just the scatterplot. Using more than one visualization can help us understand the data better!\nThat’s it for the tutorial! I hope this was helpful and thanks for reading!\n\n\n\nthanks!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Recipe Recommendaton Web App\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nJeannie Koshpasharin, Caleb Williams, Arya Shah\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Fake News with Keras\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 2, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Neural Network for Cat & Dog Image Classification\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Heat Diffusion in 2D\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nWeb App Message Bank with Dash\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nRecommending Movies with Web-scraped Data\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nOct 30, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nNOAA Climate Data Graphics\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nOct 24, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer’s Penguins Visualization Tutorial\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nCaleb Williams\n\n\n\n\n\n\nNo matching items"
  }
]